{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"“11_deep_learning.ipynb”的副本","version":"0.3.2","provenance":[{"file_id":"https://github.com/wrk1231/handson-ml/blob/master/11_deep_learning.ipynb","timestamp":1555870300111}],"collapsed_sections":["0ko9WcgOGDS-","AUjlu8lmGDTc","XouHHM28GDTd","_OJnJCPlGDUu","N4OWNbFXGDVZ","FC4e65mIGDWA","ptODkL0ZGDWK","lsSReQVOGDWO","t3CmTJzTGDWV","HhH6Xo6CGDWZ","-WxQQAvxGDWb","C0gzFd6zGDWc","iSSxMCQCGDWx","FhkAmRK4GDXG","Im-lVd7gGDXO","KqMkHd2zGDX1","w7JO6t59GDX3","5BBeCIHNGDX3","_oIxu3oKGDYI","kwGOC_MBGDYb","FKezmx2WGDYp","3sPOnZxRGDY0","2VWtSSVdGDZQ","UE2u0bryGDZR","38wQVU2NGDZT","RrZ-DB--GDZp","Dybmldp-GDZx","Za9mrOeUGDZ6","I8seniQMGDaD","ebOVr4-eGDaE","FP0FnUmiGDae","RwxcuDbbGDaq","0FR4gMR7GDat"]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.5"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"n11DRCxwGDPg","colab_type":"text"},"cell_type":"markdown","source":["**Chapter 11 – Deep Learning**"]},{"metadata":{"id":"l7sr35TfGSt-","colab_type":"code","outputId":"a3787e58-6f53-4b73-9758-3bab6e7e06a5","executionInfo":{"status":"ok","timestamp":1556486666860,"user_tz":240,"elapsed":8996,"user":{"displayName":"Ruikang Wang","photoUrl":"https://lh6.googleusercontent.com/-sxpbEWf-1Ng/AAAAAAAAAAI/AAAAAAAAAJ4/3m9qUw3an_4/s64/photo.jpg","userId":"10914323651690992949"}},"colab":{"base_uri":"https://localhost:8080/","height":104}},"cell_type":"code","source":["!git clone https://github.com/wrk1231/handson-ml\n","!mv handson-ml/* /content"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Cloning into 'handson-ml'...\n","remote: Enumerating objects: 1351, done.\u001b[K\n","remote: Total 1351 (delta 0), reused 0 (delta 0), pack-reused 1351\u001b[K\n","Receiving objects: 100% (1351/1351), 73.59 MiB | 31.50 MiB/s, done.\n","Resolving deltas: 100% (771/771), done.\n"],"name":"stdout"}]},{"metadata":{"id":"lR4k4CA1GDPi","colab_type":"text"},"cell_type":"markdown","source":["_This notebook contains all the sample code and solutions to the exercises in chapter 11._"]},{"metadata":{"id":"ZF01njCBGDPj","colab_type":"text"},"cell_type":"markdown","source":["# Setup"]},{"metadata":{"id":"1ynNXxR5GDPl","colab_type":"text"},"cell_type":"markdown","source":["First, let's make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:"]},{"metadata":{"id":"ugJ5IgR1r65l","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"EhvMVQ44GDPm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"cd2e45ee-8520-45ff-f67f-14a59667c1e1","executionInfo":{"status":"ok","timestamp":1556489402921,"user_tz":240,"elapsed":273,"user":{"displayName":"Ruikang Wang","photoUrl":"https://lh6.googleusercontent.com/-sxpbEWf-1Ng/AAAAAAAAAAI/AAAAAAAAAJ4/3m9qUw3an_4/s64/photo.jpg","userId":"10914323651690992949"}}},"cell_type":"code","source":["%%time\n","# To support both python 2 and python 3\n","from __future__ import division, print_function, unicode_literals\n","\n","# Common imports\n","import numpy as np\n","import os\n","\n","# to make this notebook's output stable across runs\n","def reset_graph(seed=42):\n","    tf.reset_default_graph()\n","    tf.set_random_seed(seed)\n","    np.random.seed(seed)\n","\n","# To plot pretty figures\n","%matplotlib inline\n","import matplotlib\n","import matplotlib.pyplot as plt\n","plt.rcParams['axes.labelsize'] = 14\n","plt.rcParams['xtick.labelsize'] = 12\n","plt.rcParams['ytick.labelsize'] = 12\n","\n","# Where to save the figures\n","PROJECT_ROOT_DIR = \".\"\n","CHAPTER_ID = \"deep\"\n","\n","def save_fig(fig_id, tight_layout=True):\n","    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n","    print(\"Saving figure\", fig_id)\n","    if tight_layout:\n","        plt.tight_layout()\n","    plt.savefig(path, format='png', dpi=300)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["CPU times: user 1.13 ms, sys: 0 ns, total: 1.13 ms\n","Wall time: 1.14 ms\n"],"name":"stdout"}]},{"metadata":{"id":"t8N_FzSKGDPq","colab_type":"text"},"cell_type":"markdown","source":["# Vanishing/Exploding Gradients Problem"]},{"metadata":{"id":"RN-1oOZvGDPr","colab_type":"code","colab":{}},"cell_type":"code","source":["def logit(z):\n","    return 1 / (1 + np.exp(-z))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-i38RUyaGDPw","colab_type":"code","outputId":"5efca645-07e6-4ada-eaf9-fa50edfe71d6","executionInfo":{"status":"ok","timestamp":1556489406306,"user_tz":240,"elapsed":1051,"user":{"displayName":"Ruikang Wang","photoUrl":"https://lh6.googleusercontent.com/-sxpbEWf-1Ng/AAAAAAAAAAI/AAAAAAAAAJ4/3m9qUw3an_4/s64/photo.jpg","userId":"10914323651690992949"}},"colab":{"base_uri":"https://localhost:8080/","height":315}},"cell_type":"code","source":["z = np.linspace(-5, 5, 200)\n","\n","plt.plot([-5, 5], [0, 0], 'k-')\n","plt.plot([-5, 5], [1, 1], 'k--')\n","plt.plot([0, 0], [-0.2, 1.2], 'k-')\n","plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n","plt.plot(z, logit(z), \"b-\", linewidth=2)\n","props = dict(facecolor='black', shrink=0.1)\n","plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n","plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n","plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n","plt.grid(True)\n","plt.title(\"Sigmoid activation function\", fontsize=14)\n","plt.axis([-5, 5, -0.2, 1.2])\n","\n","save_fig(\"sigmoid_saturation_plot\")\n","plt.show()"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Saving figure sigmoid_saturation_plot\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkpSgREQkkpSgREQkkv4fpnIt6Q3iZsAAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"1VeY2vxeGDP1","colab_type":"text"},"cell_type":"markdown","source":["## Xavier and He Initialization"]},{"metadata":{"id":"szRBC_ZfGDP2","colab_type":"text"},"cell_type":"markdown","source":["Note: the book uses `tensorflow.contrib.layers.fully_connected()` rather than `tf.layers.dense()` (which did not exist when this chapter was written). It is now preferable to use `tf.layers.dense()`, because anything in the contrib module may change or be deleted without notice. The `dense()` function is almost identical to the `fully_connected()` function. The main differences relevant to this chapter are:\n","* several parameters are renamed: `scope` becomes `name`, `activation_fn` becomes `activation` (and similarly the `_fn` suffix is removed from other parameters such as `normalizer_fn`), `weights_initializer` becomes `kernel_initializer`, etc.\n","* the default `activation` is now `None` rather than `tf.nn.relu`.\n","* it does not support `tensorflow.contrib.framework.arg_scope()` (introduced later in chapter 11).\n","* it does not support regularizer params (introduced later in chapter 11)."]},{"metadata":{"id":"RieujpITGDP3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"f7924672-67e4-4689-cae9-53ed128a9e34","executionInfo":{"status":"ok","timestamp":1556487708423,"user_tz":240,"elapsed":319,"user":{"displayName":"Ruikang Wang","photoUrl":"https://lh6.googleusercontent.com/-sxpbEWf-1Ng/AAAAAAAAAAI/AAAAAAAAAJ4/3m9qUw3an_4/s64/photo.jpg","userId":"10914323651690992949"}}},"cell_type":"code","source":["%%time\n","import tensorflow as tf"],"execution_count":12,"outputs":[{"output_type":"stream","text":["CPU times: user 7 µs, sys: 0 ns, total: 7 µs\n","Wall time: 10.5 µs\n"],"name":"stdout"}]},{"metadata":{"id":"2eheShnbGDP6","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 28 * 28  # MNIST\n","n_hidden1 = 300\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"imp-GdUOGDP9","colab_type":"code","colab":{}},"cell_type":"code","source":["he_init = tf.variance_scaling_initializer()\n","hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n","                          kernel_initializer=he_init, name=\"hidden1\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VBd0R0PtGDQB","colab_type":"text"},"cell_type":"markdown","source":["## Nonsaturating Activation Functions"]},{"metadata":{"id":"_-EwpsOnGDQC","colab_type":"text"},"cell_type":"markdown","source":["### Leaky ReLU"]},{"metadata":{"id":"1NwB4Ds3GDQE","colab_type":"code","colab":{}},"cell_type":"code","source":["def leaky_relu(z, alpha=0.01):\n","    return np.maximum(alpha*z, z)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ci1HRNSnGDQJ","colab_type":"code","outputId":"93195a04-7774-4be6-df69-5fd7b52dcc39","executionInfo":{"status":"ok","timestamp":1556489753551,"user_tz":240,"elapsed":944,"user":{"displayName":"Ruikang Wang","photoUrl":"https://lh6.googleusercontent.com/-sxpbEWf-1Ng/AAAAAAAAAAI/AAAAAAAAAJ4/3m9qUw3an_4/s64/photo.jpg","userId":"10914323651690992949"}},"colab":{"base_uri":"https://localhost:8080/","height":315}},"cell_type":"code","source":["plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n","plt.plot([-5, 5], [0, 0], 'k-')\n","plt.plot([0, 0], [-0.5, 4.2], 'k-')\n","plt.grid(True)\n","props = dict(facecolor='black', shrink=0.1)\n","plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n","plt.title(\"Leaky ReLU activation function\", fontsize=14)\n","plt.axis([-5, 5, -0.5, 4.2])\n","\n","save_fig(\"leaky_relu_plot\")\n","plt.show()"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Saving figure leaky_relu_plot\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/h9r5scSI6iwhAAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"dJhLfnLtGDQN","colab_type":"text"},"cell_type":"markdown","source":["Implementing Leaky ReLU in TensorFlow:"]},{"metadata":{"id":"zd4Yrc3ZGDQP","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"B2PjamU5GDQS","colab_type":"code","colab":{}},"cell_type":"code","source":["def leaky_relu(z, name=None):\n","    return tf.maximum(0.01 * z, z, name=name)\n","\n","hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"t-CrwfKYGDQU","colab_type":"text"},"cell_type":"markdown","source":["Let's train a neural network on MNIST using the Leaky ReLU. First let's create the graph:"]},{"metadata":{"id":"IHRSF90OGDQV","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 28 * 28  # MNIST\n","n_hidden1 = 300\n","n_hidden2 = 100\n","n_outputs = 10"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cl1lIJQqGDQY","colab_type":"code","colab":{}},"cell_type":"code","source":["X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"N2DkQQeDGDQb","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope(\"dnn\"):\n","    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")\n","    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu, name=\"hidden2\")\n","    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vmThavBzGDQq","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope(\"loss\"):\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","    loss = tf.reduce_mean(xentropy, name=\"loss\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YB_XPS95GDQx","colab_type":"code","colab":{}},"cell_type":"code","source":["learning_rate = 0.01\n","\n","with tf.name_scope(\"train\"):\n","    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","    training_op = optimizer.minimize(loss)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3MKCMR7pGDQ0","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xgzpzp7BGDQ7","colab_type":"code","colab":{}},"cell_type":"code","source":["init = tf.global_variables_initializer()\n","saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"frwYaDYXGDRC","colab_type":"text"},"cell_type":"markdown","source":["Let's load the data:"]},{"metadata":{"id":"9gX7zZRuGDRC","colab_type":"text"},"cell_type":"markdown","source":["**Warning**: `tf.examples.tutorials.mnist` is deprecated. We will use `tf.keras.datasets.mnist` instead."]},{"metadata":{"id":"f8oSFbcLGDRD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"5613e888-8c64-4c8b-f5ef-54102ed5531c","executionInfo":{"status":"ok","timestamp":1556489785806,"user_tz":240,"elapsed":1184,"user":{"displayName":"Ruikang Wang","photoUrl":"https://lh6.googleusercontent.com/-sxpbEWf-1Ng/AAAAAAAAAAI/AAAAAAAAAJ4/3m9qUw3an_4/s64/photo.jpg","userId":"10914323651690992949"}}},"cell_type":"code","source":["(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n","X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n","X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n","y_train = y_train.astype(np.int32)\n","y_test = y_test.astype(np.int32)\n","X_valid, X_train = X_train[:5000], X_train[5000:]\n","y_valid, y_train = y_train[:5000], y_train[5000:]"],"execution_count":32,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"metadata":{"id":"VsXI5UuhGDRF","colab_type":"code","colab":{}},"cell_type":"code","source":["def shuffle_batch(X, y, batch_size):\n","    rnd_idx = np.random.permutation(len(X))\n","    n_batches = len(X) // batch_size\n","    for batch_idx in np.array_split(rnd_idx, n_batches):\n","        X_batch, y_batch = X[batch_idx], y[batch_idx]\n","        yield X_batch, y_batch"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":true,"id":"6T5HnTD8GDRH","colab_type":"code","outputId":"814e554f-7ad3-498e-f8cd-67dcc053e6e9","executionInfo":{"status":"ok","timestamp":1556489910862,"user_tz":240,"elapsed":123088,"user":{"displayName":"Ruikang Wang","photoUrl":"https://lh6.googleusercontent.com/-sxpbEWf-1Ng/AAAAAAAAAAI/AAAAAAAAAJ4/3m9qUw3an_4/s64/photo.jpg","userId":"10914323651690992949"}},"colab":{"base_uri":"https://localhost:8080/","height":155}},"cell_type":"code","source":["n_epochs = 40\n","batch_size = 50\n","\n","with tf.Session() as sess:\n","    init.run()\n","    for epoch in range(n_epochs):\n","        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","        if epoch % 5 == 0:\n","            acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n","            acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n","            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n","\n","    save_path = saver.save(sess, \"./my_model_final.ckpt\")"],"execution_count":34,"outputs":[{"output_type":"stream","text":["0 Batch accuracy: 0.86 Validation accuracy: 0.9048\n","5 Batch accuracy: 0.94 Validation accuracy: 0.9496\n","10 Batch accuracy: 0.92 Validation accuracy: 0.9654\n","15 Batch accuracy: 0.94 Validation accuracy: 0.9712\n","20 Batch accuracy: 1.0 Validation accuracy: 0.9762\n","25 Batch accuracy: 1.0 Validation accuracy: 0.9778\n","30 Batch accuracy: 0.98 Validation accuracy: 0.978\n","35 Batch accuracy: 1.0 Validation accuracy: 0.9786\n"],"name":"stdout"}]},{"metadata":{"id":"bX5OinQpGDRM","colab_type":"text"},"cell_type":"markdown","source":["### ELU"]},{"metadata":{"id":"Woo218t2GDRN","colab_type":"code","colab":{}},"cell_type":"code","source":["def elu(z, alpha=1):\n","    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"whkc_RBpGDRQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":315},"outputId":"64dd0158-bc4b-42eb-f172-ab97ff4f60d9","executionInfo":{"status":"ok","timestamp":1556490694158,"user_tz":240,"elapsed":1046,"user":{"displayName":"Ruikang Wang","photoUrl":"https://lh6.googleusercontent.com/-sxpbEWf-1Ng/AAAAAAAAAAI/AAAAAAAAAJ4/3m9qUw3an_4/s64/photo.jpg","userId":"10914323651690992949"}}},"cell_type":"code","source":["plt.plot(z, elu(z), \"b-\", linewidth=2)\n","plt.plot([-5, 5], [0, 0], 'k-')\n","plt.plot([-5, 5], [-1, -1], 'k--')\n","plt.plot([0, 0], [-2.2, 3.2], 'k-')\n","plt.grid(True)\n","plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n","plt.axis([-5, 5, -2.2, 3.2])\n","\n","save_fig(\"elu_plot\")\n","plt.show()"],"execution_count":36,"outputs":[{"output_type":"stream","text":["Saving figure elu_plot\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSE/diHEg1QAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"9Wp0ChxXGDRU","colab_type":"text"},"cell_type":"markdown","source":["Implementing ELU in TensorFlow is trivial, just specify the activation function when building each layer:"]},{"metadata":{"id":"1kixihRAGDRW","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1HNMiiOkGDRa","colab_type":"code","colab":{}},"cell_type":"code","source":["hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name=\"hidden1\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2PhI542TGDRd","colab_type":"text"},"cell_type":"markdown","source":["### SELU"]},{"metadata":{"id":"fITqIjoNGDRe","colab_type":"text"},"cell_type":"markdown","source":["This activation function was proposed in this [great paper](https://arxiv.org/pdf/1706.02515.pdf) by Günter Klambauer, Thomas Unterthiner and Andreas Mayr, published in June 2017 (I will definitely add it to the book). During training, a neural network composed of a stack of dense layers using the SELU activation function will self-normalize: the output of each layer will tend to preserve the same mean and variance during training, which solves the vanishing/exploding gradients problem. As a result, this activation function outperforms the other activation functions very significantly for such neural nets, so you should really try it out."]},{"metadata":{"id":"rdXUzP-KGDRf","colab_type":"code","colab":{}},"cell_type":"code","source":["def selu(z,\n","         scale=1.0507009873554804934193349852946,\n","         alpha=1.6732632423543772848170429916717):\n","    return scale * elu(z, alpha)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FyWfuZwdGDRi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":315},"outputId":"41dd8920-6795-48c0-dfd9-86b2eeee4696","executionInfo":{"status":"ok","timestamp":1556490701718,"user_tz":240,"elapsed":966,"user":{"displayName":"Ruikang Wang","photoUrl":"https://lh6.googleusercontent.com/-sxpbEWf-1Ng/AAAAAAAAAAI/AAAAAAAAAJ4/3m9qUw3an_4/s64/photo.jpg","userId":"10914323651690992949"}}},"cell_type":"code","source":["plt.plot(z, selu(z), \"b-\", linewidth=2)\n","plt.plot([-5, 5], [0, 0], 'k-')\n","plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n","plt.plot([0, 0], [-2.2, 3.2], 'k-')\n","plt.grid(True)\n","plt.title(r\"SELU activation function\", fontsize=14)\n","plt.axis([-5, 5, -2.2, 3.2])\n","\n","save_fig(\"selu_plot\")\n","plt.show()"],"execution_count":40,"outputs":[{"output_type":"stream","text":["Saving figure selu_plot\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUCIiEkkKKBERiaT/B9c9MCs0b4SXAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"fqoBU53kGDRq","colab_type":"text"},"cell_type":"markdown","source":["By default, the SELU hyperparameters (`scale` and `alpha`) are tuned in such a way that the mean remains close to 0, and the standard deviation remains close to 1 (assuming the inputs are standardized with mean 0 and standard deviation 1 too). Using this activation function, even a 100 layer deep neural network preserves roughly mean 0 and standard deviation 1 across all layers, avoiding the exploding/vanishing gradients problem:"]},{"metadata":{"id":"EYohJK8fGDRq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":190},"outputId":"0f2b7362-d09c-49a0-ce56-918bd86b3d38","executionInfo":{"status":"ok","timestamp":1556490729441,"user_tz":240,"elapsed":586,"user":{"displayName":"Ruikang Wang","photoUrl":"https://lh6.googleusercontent.com/-sxpbEWf-1Ng/AAAAAAAAAAI/AAAAAAAAAJ4/3m9qUw3an_4/s64/photo.jpg","userId":"10914323651690992949"}}},"cell_type":"code","source":["np.random.seed(42)\n","Z = np.random.normal(size=(500, 100))\n","for layer in range(100):\n","    W = np.random.normal(size=(100, 100), scale=np.sqrt(1/100))\n","    Z = selu(np.dot(Z, W))\n","    means = np.mean(Z, axis=1)\n","    stds = np.std(Z, axis=1)\n","    if layer % 10 == 0:\n","        print(\"Layer {}: {:.2f} < mean < {:.2f}, {:.2f} < std deviation < {:.2f}\".format(\n","            layer, means.min(), means.max(), stds.min(), stds.max()))"],"execution_count":41,"outputs":[{"output_type":"stream","text":["Layer 0: -0.26 < mean < 0.27, 0.74 < std deviation < 1.27\n","Layer 10: -0.24 < mean < 0.27, 0.74 < std deviation < 1.27\n","Layer 20: -0.17 < mean < 0.18, 0.74 < std deviation < 1.24\n","Layer 30: -0.27 < mean < 0.24, 0.78 < std deviation < 1.20\n","Layer 40: -0.38 < mean < 0.39, 0.74 < std deviation < 1.25\n","Layer 50: -0.27 < mean < 0.31, 0.73 < std deviation < 1.27\n","Layer 60: -0.26 < mean < 0.43, 0.74 < std deviation < 1.35\n","Layer 70: -0.19 < mean < 0.21, 0.75 < std deviation < 1.21\n","Layer 80: -0.18 < mean < 0.16, 0.72 < std deviation < 1.19\n","Layer 90: -0.19 < mean < 0.16, 0.75 < std deviation < 1.20\n"],"name":"stdout"}]},{"metadata":{"id":"ErwktmQ7GDRt","colab_type":"text"},"cell_type":"markdown","source":["The `tf.nn.selu()` function was added in TensorFlow 1.4. For earlier versions, you can use the following implementation:"]},{"metadata":{"id":"kgRW-BpfGDRu","colab_type":"code","colab":{}},"cell_type":"code","source":["def selu(z,\n","         scale=1.0507009873554804934193349852946,\n","         alpha=1.6732632423543772848170429916717):\n","    return scale * tf.where(z >= 0.0, z, alpha * tf.nn.elu(z))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-UeOuSFSGDR1","colab_type":"text"},"cell_type":"markdown","source":["However, the SELU activation function cannot be used along with regular Dropout (this would cancel the SELU activation function's self-normalizing property). Fortunately, there is a Dropout variant called Alpha Dropout proposed in the same paper. It is available in `tf.contrib.nn.alpha_dropout()` since TF 1.4 (or check out [this implementation](https://github.com/bioinf-jku/SNNs/blob/master/selu.py) by the Institute of Bioinformatics, Johannes Kepler University Linz)."]},{"metadata":{"id":"KRuF17vnGDR8","colab_type":"text"},"cell_type":"markdown","source":["Let's create a neural net for MNIST using the SELU activation function:"]},{"metadata":{"id":"yORghFHWGDR-","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 28 * 28  # MNIST\n","n_hidden1 = 300\n","n_hidden2 = 100\n","n_outputs = 10\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n","\n","with tf.name_scope(\"dnn\"):\n","    hidden1 = tf.layers.dense(X, n_hidden1, activation=selu, name=\"hidden1\")\n","    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=selu, name=\"hidden2\")\n","    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n","\n","with tf.name_scope(\"loss\"):\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","    loss = tf.reduce_mean(xentropy, name=\"loss\")\n","\n","learning_rate = 0.01\n","\n","with tf.name_scope(\"train\"):\n","    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","    training_op = optimizer.minimize(loss)\n","\n","with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n","\n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()\n","n_epochs = 40\n","batch_size = 50"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dwGls3gQGDSB","colab_type":"text"},"cell_type":"markdown","source":["Now let's train it. Do not forget to scale the inputs to mean 0 and standard deviation 1:"]},{"metadata":{"id":"iVyzbU6sGDSC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":155},"outputId":"908dff09-65d2-443b-a0ea-ecaf0acb598e","executionInfo":{"status":"ok","timestamp":1556490905897,"user_tz":240,"elapsed":129170,"user":{"displayName":"Ruikang Wang","photoUrl":"https://lh6.googleusercontent.com/-sxpbEWf-1Ng/AAAAAAAAAAI/AAAAAAAAAJ4/3m9qUw3an_4/s64/photo.jpg","userId":"10914323651690992949"}}},"cell_type":"code","source":["means = X_train.mean(axis=0, keepdims=True)\n","stds = X_train.std(axis=0, keepdims=True) + 1e-10\n","X_val_scaled = (X_valid - means) / stds\n","\n","with tf.Session() as sess:\n","    init.run()\n","    for epoch in range(n_epochs):\n","        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","            X_batch_scaled = (X_batch - means) / stds\n","            sess.run(training_op, feed_dict={X: X_batch_scaled, y: y_batch})\n","        if epoch % 5 == 0:\n","            acc_batch = accuracy.eval(feed_dict={X: X_batch_scaled, y: y_batch})\n","            acc_valid = accuracy.eval(feed_dict={X: X_val_scaled, y: y_valid})\n","            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n","\n","    save_path = saver.save(sess, \"./my_model_final_selu.ckpt\")"],"execution_count":44,"outputs":[{"output_type":"stream","text":["0 Batch accuracy: 0.88 Validation accuracy: 0.9232\n","5 Batch accuracy: 0.98 Validation accuracy: 0.9574\n","10 Batch accuracy: 1.0 Validation accuracy: 0.9658\n","15 Batch accuracy: 0.96 Validation accuracy: 0.9684\n","20 Batch accuracy: 1.0 Validation accuracy: 0.9692\n","25 Batch accuracy: 1.0 Validation accuracy: 0.969\n","30 Batch accuracy: 1.0 Validation accuracy: 0.969\n","35 Batch accuracy: 1.0 Validation accuracy: 0.9698\n"],"name":"stdout"}]},{"metadata":{"id":"F4Tz9AIOGDSJ","colab_type":"text"},"cell_type":"markdown","source":["# Batch Normalization"]},{"metadata":{"id":"Zaury8INGDSK","colab_type":"text"},"cell_type":"markdown","source":["Note: the book uses `tensorflow.contrib.layers.batch_norm()` rather than `tf.layers.batch_normalization()` (which did not exist when this chapter was written). It is now preferable to use `tf.layers.batch_normalization()`, because anything in the contrib module may change or be deleted without notice. Instead of using the `batch_norm()` function as a regularizer parameter to the `fully_connected()` function, we now use `batch_normalization()` and we explicitly create a distinct layer. The parameters are a bit different, in particular:\n","* `decay` is renamed to `momentum`,\n","* `is_training` is renamed to `training`,\n","* `updates_collections` is removed: the update operations needed by batch normalization are added to the `UPDATE_OPS` collection and you need to explicity run these operations during training (see the execution phase below),\n","* we don't need to specify `scale=True`, as that is the default.\n","\n","Also note that in order to run batch norm just _before_ each hidden layer's activation function, we apply the ELU activation function manually, right after the batch norm layer.\n","\n","Note: since the `tf.layers.dense()` function is incompatible with `tf.contrib.layers.arg_scope()` (which is used in the book), we now use python's `functools.partial()` function instead. It makes it easy to create a `my_dense_layer()` function that just calls `tf.layers.dense()` with the desired parameters automatically set (unless they are overridden when calling `my_dense_layer()`). As you can see, the code remains very similar."]},{"metadata":{"id":"hIVYECkrGDSK","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","import tensorflow as tf\n","\n","n_inputs = 28 * 28\n","n_hidden1 = 300\n","n_hidden2 = 100\n","n_outputs = 10\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","\n","training = tf.placeholder_with_default(False, shape=(), name='training')\n","\n","hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n","bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n","bn1_act = tf.nn.elu(bn1)\n","\n","hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n","bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n","bn2_act = tf.nn.elu(bn2)\n","\n","logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n","logits = tf.layers.batch_normalization(logits_before_bn, training=training,\n","                                       momentum=0.9)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yK1gT_JSGDSO","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","training = tf.placeholder_with_default(False, shape=(), name='training')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UGY4LDdFGDSR","colab_type":"text"},"cell_type":"markdown","source":["To avoid repeating the same parameters over and over again, we can use Python's `partial()` function:"]},{"metadata":{"id":"Mp6xUqJmGDSS","colab_type":"code","colab":{}},"cell_type":"code","source":["from functools import partial\n","\n","my_batch_norm_layer = partial(tf.layers.batch_normalization,\n","                              training=training, momentum=0.9)\n","\n","hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n","bn1 = my_batch_norm_layer(hidden1)\n","bn1_act = tf.nn.elu(bn1)\n","hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n","bn2 = my_batch_norm_layer(hidden2)\n","bn2_act = tf.nn.elu(bn2)\n","logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n","logits = my_batch_norm_layer(logits_before_bn)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"b4yt4rUxGDSY","colab_type":"text"},"cell_type":"markdown","source":["Let's build a neural net for MNIST, using the ELU activation function and Batch Normalization at each layer:"]},{"metadata":{"id":"vZrbTbLWGDSY","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","batch_norm_momentum = 0.9\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n","training = tf.placeholder_with_default(False, shape=(), name='training')\n","\n","with tf.name_scope(\"dnn\"):\n","    he_init = tf.variance_scaling_initializer()\n","\n","    my_batch_norm_layer = partial(\n","            tf.layers.batch_normalization,\n","            training=training,\n","            momentum=batch_norm_momentum)\n","\n","    my_dense_layer = partial(\n","            tf.layers.dense,\n","            kernel_initializer=he_init)\n","\n","    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n","    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n","    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden2\")\n","    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n","    logits_before_bn = my_dense_layer(bn2, n_outputs, name=\"outputs\")\n","    logits = my_batch_norm_layer(logits_before_bn)\n","\n","with tf.name_scope(\"loss\"):\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","    loss = tf.reduce_mean(xentropy, name=\"loss\")\n","\n","with tf.name_scope(\"train\"):\n","    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","    training_op = optimizer.minimize(loss)\n","\n","with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n","    \n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_bcxvQZ_GDSh","colab_type":"text"},"cell_type":"markdown","source":["Note: since we are using `tf.layers.batch_normalization()` rather than `tf.contrib.layers.batch_norm()` (as in the book), we need to explicitly run the extra update operations needed by batch normalization (`sess.run([training_op, extra_update_ops],...`)."]},{"metadata":{"id":"8a_0GxbxGDSm","colab_type":"code","colab":{}},"cell_type":"code","source":["n_epochs = 20\n","batch_size = 200"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zGWc0A7xGDSq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":363},"outputId":"36a30710-1d7f-47d5-fb69-fc9af0196918","executionInfo":{"status":"ok","timestamp":1556498539657,"user_tz":240,"elapsed":47584,"user":{"displayName":"Ruikang Wang","photoUrl":"https://lh6.googleusercontent.com/-sxpbEWf-1Ng/AAAAAAAAAAI/AAAAAAAAAJ4/3m9qUw3an_4/s64/photo.jpg","userId":"10914323651690992949"}}},"cell_type":"code","source":["extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n","\n","with tf.Session() as sess:\n","    init.run()\n","    for epoch in range(n_epochs):\n","        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","            sess.run([training_op, extra_update_ops],\n","                     feed_dict={training: True, X: X_batch, y: y_batch})\n","        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n","        print(epoch, \"Validation accuracy:\", accuracy_val)\n","\n","    save_path = saver.save(sess, \"./my_model_final.ckpt\")"],"execution_count":52,"outputs":[{"output_type":"stream","text":["0 Validation accuracy: 0.8952\n","1 Validation accuracy: 0.9202\n","2 Validation accuracy: 0.9318\n","3 Validation accuracy: 0.9422\n","4 Validation accuracy: 0.9468\n","5 Validation accuracy: 0.954\n","6 Validation accuracy: 0.9568\n","7 Validation accuracy: 0.96\n","8 Validation accuracy: 0.962\n","9 Validation accuracy: 0.9638\n","10 Validation accuracy: 0.9662\n","11 Validation accuracy: 0.9682\n","12 Validation accuracy: 0.9672\n","13 Validation accuracy: 0.9696\n","14 Validation accuracy: 0.9706\n","15 Validation accuracy: 0.9704\n","16 Validation accuracy: 0.9718\n","17 Validation accuracy: 0.9726\n","18 Validation accuracy: 0.9738\n","19 Validation accuracy: 0.9742\n"],"name":"stdout"}]},{"metadata":{"id":"44J6jJspGDSu","colab_type":"text"},"cell_type":"markdown","source":["What!? That's not a great accuracy for MNIST. Of course, if you train for longer it will get much better accuracy, but with such a shallow network, Batch Norm and ELU are unlikely to have very positive impact: they shine mostly for much deeper nets."]},{"metadata":{"id":"Milil5frGDSv","colab_type":"text"},"cell_type":"markdown","source":["Note that you could also make the training operation depend on the update operations:\n","\n","```python\n","with tf.name_scope(\"train\"):\n","    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n","    with tf.control_dependencies(extra_update_ops):\n","        training_op = optimizer.minimize(loss)\n","```\n","\n","This way, you would just have to evaluate the `training_op` during training, TensorFlow would automatically run the update operations as well:\n","\n","```python\n","sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})\n","```"]},{"metadata":{"id":"qYzIgz58GDSw","colab_type":"text"},"cell_type":"markdown","source":["One more thing: notice that the list of trainable variables is shorter than the list of all global variables. This is because the moving averages are non-trainable variables. If you want to reuse a pretrained neural network (see below), you must not forget these non-trainable variables."]},{"metadata":{"id":"Z8fH8-XIGDSy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":225},"outputId":"442d23bf-e289-4da6-802a-85515fb62362","executionInfo":{"status":"ok","timestamp":1556499101558,"user_tz":240,"elapsed":279,"user":{"displayName":"Ruikang Wang","photoUrl":"https://lh6.googleusercontent.com/-sxpbEWf-1Ng/AAAAAAAAAAI/AAAAAAAAAJ4/3m9qUw3an_4/s64/photo.jpg","userId":"10914323651690992949"}}},"cell_type":"code","source":["[v.name for v in tf.trainable_variables()]"],"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['hidden1/kernel:0',\n"," 'hidden1/bias:0',\n"," 'batch_normalization/gamma:0',\n"," 'batch_normalization/beta:0',\n"," 'hidden2/kernel:0',\n"," 'hidden2/bias:0',\n"," 'batch_normalization_1/gamma:0',\n"," 'batch_normalization_1/beta:0',\n"," 'outputs/kernel:0',\n"," 'outputs/bias:0',\n"," 'batch_normalization_2/gamma:0',\n"," 'batch_normalization_2/beta:0']"]},"metadata":{"tags":[]},"execution_count":54}]},{"metadata":{"id":"LcRWXzh6GDS3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":328},"outputId":"086e8022-89ff-4869-84f8-c6a6dab79e2f","executionInfo":{"status":"ok","timestamp":1556499127206,"user_tz":240,"elapsed":289,"user":{"displayName":"Ruikang Wang","photoUrl":"https://lh6.googleusercontent.com/-sxpbEWf-1Ng/AAAAAAAAAAI/AAAAAAAAAJ4/3m9qUw3an_4/s64/photo.jpg","userId":"10914323651690992949"}}},"cell_type":"code","source":["[v.name for v in tf.global_variables()]"],"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['hidden1/kernel:0',\n"," 'hidden1/bias:0',\n"," 'batch_normalization/gamma:0',\n"," 'batch_normalization/beta:0',\n"," 'batch_normalization/moving_mean:0',\n"," 'batch_normalization/moving_variance:0',\n"," 'hidden2/kernel:0',\n"," 'hidden2/bias:0',\n"," 'batch_normalization_1/gamma:0',\n"," 'batch_normalization_1/beta:0',\n"," 'batch_normalization_1/moving_mean:0',\n"," 'batch_normalization_1/moving_variance:0',\n"," 'outputs/kernel:0',\n"," 'outputs/bias:0',\n"," 'batch_normalization_2/gamma:0',\n"," 'batch_normalization_2/beta:0',\n"," 'batch_normalization_2/moving_mean:0',\n"," 'batch_normalization_2/moving_variance:0']"]},"metadata":{"tags":[]},"execution_count":56}]},{"metadata":{"id":"0ko9WcgOGDS-","colab_type":"text"},"cell_type":"markdown","source":["## Gradient Clipping"]},{"metadata":{"id":"vSsimaU4GDS-","colab_type":"text"},"cell_type":"markdown","source":["Let's create a simple neural net for MNIST and add gradient clipping. The first part is the same as earlier (except we added a few more layers to demonstrate reusing pretrained models, see below):"]},{"metadata":{"id":"HyigJjioGDTA","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 28 * 28  # MNIST\n","n_hidden1 = 300\n","n_hidden2 = 50\n","n_hidden3 = 50\n","n_hidden4 = 50\n","n_hidden5 = 50\n","n_outputs = 10\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n","\n","with tf.name_scope(\"dnn\"):\n","    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n","    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n","    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n","    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n","    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n","    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n","\n","with tf.name_scope(\"loss\"):\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","    loss = tf.reduce_mean(xentropy, name=\"loss\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cZ4axtdYGDTC","colab_type":"code","colab":{}},"cell_type":"code","source":["learning_rate = 0.01"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5NUa-rSqGDTD","colab_type":"text"},"cell_type":"markdown","source":["Now we apply gradient clipping. For this, we need to get the gradients, use the `clip_by_value()` function to clip them, then apply them:"]},{"metadata":{"id":"KJLjHs3nGDTE","colab_type":"code","colab":{}},"cell_type":"code","source":["threshold = 1.0\n","\n","optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","grads_and_vars = optimizer.compute_gradients(loss)\n","capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n","              for grad, var in grads_and_vars]\n","training_op = optimizer.apply_gradients(capped_gvs)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"X72nyaQ3GDTJ","colab_type":"text"},"cell_type":"markdown","source":["The rest is the same as usual:"]},{"metadata":{"id":"tBxYilKAGDTJ","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"S6LQ3_UYGDTO","colab_type":"code","colab":{}},"cell_type":"code","source":["init = tf.global_variables_initializer()\n","saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cZiJUFCXGDTQ","colab_type":"code","colab":{}},"cell_type":"code","source":["n_epochs = 20\n","batch_size = 200"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TUOLMT28GDTT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":363},"outputId":"468a4abd-adc6-4903-c541-dfde2a5c5562","executionInfo":{"status":"ok","timestamp":1556500274830,"user_tz":240,"elapsed":37810,"user":{"displayName":"Ruikang Wang","photoUrl":"https://lh6.googleusercontent.com/-sxpbEWf-1Ng/AAAAAAAAAAI/AAAAAAAAAJ4/3m9qUw3an_4/s64/photo.jpg","userId":"10914323651690992949"}}},"cell_type":"code","source":["with tf.Session() as sess:\n","    init.run()\n","    for epoch in range(n_epochs):\n","        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n","        print(epoch, \"Validation accuracy:\", accuracy_val)\n","\n","    save_path = saver.save(sess, \"./my_model_final.ckpt\")"],"execution_count":64,"outputs":[{"output_type":"stream","text":["0 Validation accuracy: 0.2876\n","1 Validation accuracy: 0.7944\n","2 Validation accuracy: 0.8794\n","3 Validation accuracy: 0.906\n","4 Validation accuracy: 0.9162\n","5 Validation accuracy: 0.9218\n","6 Validation accuracy: 0.9292\n","7 Validation accuracy: 0.9358\n","8 Validation accuracy: 0.9378\n","9 Validation accuracy: 0.9416\n","10 Validation accuracy: 0.9454\n","11 Validation accuracy: 0.947\n","12 Validation accuracy: 0.9476\n","13 Validation accuracy: 0.953\n","14 Validation accuracy: 0.9564\n","15 Validation accuracy: 0.9566\n","16 Validation accuracy: 0.9574\n","17 Validation accuracy: 0.9588\n","18 Validation accuracy: 0.9624\n","19 Validation accuracy: 0.961\n"],"name":"stdout"}]},{"metadata":{"id":"AUjlu8lmGDTc","colab_type":"text"},"cell_type":"markdown","source":["## Reusing Pretrained Layers"]},{"metadata":{"id":"XouHHM28GDTd","colab_type":"text"},"cell_type":"markdown","source":["## Reusing a TensorFlow Model"]},{"metadata":{"id":"2Y9kq3azGDTg","colab_type":"text"},"cell_type":"markdown","source":["First you need to load the graph's structure. The `import_meta_graph()` function does just that, loading the graph's operations into the default graph, and returning a `Saver` that you can then use to restore the model's state. Note that by default, a `Saver` saves the structure of the graph into a `.meta` file, so that's the file you should load:"]},{"metadata":{"id":"2Oce2ucRGDTi","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RjHLCzPbGDTl","colab_type":"code","colab":{}},"cell_type":"code","source":["saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nCvmR49nGDTr","colab_type":"text"},"cell_type":"markdown","source":["Next you need to get a handle on all the operations you will need for training. If you don't know the graph's structure, you can list all the operations:"]},{"metadata":{"id":"Q9KRpbrmGDTr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":7341},"outputId":"8218cb89-f642-480d-b3fd-731095f736be","executionInfo":{"status":"ok","timestamp":1556500286565,"user_tz":240,"elapsed":433,"user":{"displayName":"Ruikang Wang","photoUrl":"https://lh6.googleusercontent.com/-sxpbEWf-1Ng/AAAAAAAAAAI/AAAAAAAAAJ4/3m9qUw3an_4/s64/photo.jpg","userId":"10914323651690992949"}}},"cell_type":"code","source":["for op in tf.get_default_graph().get_operations():\n","    print(op.name)"],"execution_count":67,"outputs":[{"output_type":"stream","text":["X\n","y\n","hidden1/kernel/Initializer/random_uniform/shape\n","hidden1/kernel/Initializer/random_uniform/min\n","hidden1/kernel/Initializer/random_uniform/max\n","hidden1/kernel/Initializer/random_uniform/RandomUniform\n","hidden1/kernel/Initializer/random_uniform/sub\n","hidden1/kernel/Initializer/random_uniform/mul\n","hidden1/kernel/Initializer/random_uniform\n","hidden1/kernel\n","hidden1/kernel/Assign\n","hidden1/kernel/read\n","hidden1/bias/Initializer/zeros\n","hidden1/bias\n","hidden1/bias/Assign\n","hidden1/bias/read\n","dnn/hidden1/MatMul\n","dnn/hidden1/BiasAdd\n","dnn/hidden1/Relu\n","hidden2/kernel/Initializer/random_uniform/shape\n","hidden2/kernel/Initializer/random_uniform/min\n","hidden2/kernel/Initializer/random_uniform/max\n","hidden2/kernel/Initializer/random_uniform/RandomUniform\n","hidden2/kernel/Initializer/random_uniform/sub\n","hidden2/kernel/Initializer/random_uniform/mul\n","hidden2/kernel/Initializer/random_uniform\n","hidden2/kernel\n","hidden2/kernel/Assign\n","hidden2/kernel/read\n","hidden2/bias/Initializer/zeros\n","hidden2/bias\n","hidden2/bias/Assign\n","hidden2/bias/read\n","dnn/hidden2/MatMul\n","dnn/hidden2/BiasAdd\n","dnn/hidden2/Relu\n","hidden3/kernel/Initializer/random_uniform/shape\n","hidden3/kernel/Initializer/random_uniform/min\n","hidden3/kernel/Initializer/random_uniform/max\n","hidden3/kernel/Initializer/random_uniform/RandomUniform\n","hidden3/kernel/Initializer/random_uniform/sub\n","hidden3/kernel/Initializer/random_uniform/mul\n","hidden3/kernel/Initializer/random_uniform\n","hidden3/kernel\n","hidden3/kernel/Assign\n","hidden3/kernel/read\n","hidden3/bias/Initializer/zeros\n","hidden3/bias\n","hidden3/bias/Assign\n","hidden3/bias/read\n","dnn/hidden3/MatMul\n","dnn/hidden3/BiasAdd\n","dnn/hidden3/Relu\n","hidden4/kernel/Initializer/random_uniform/shape\n","hidden4/kernel/Initializer/random_uniform/min\n","hidden4/kernel/Initializer/random_uniform/max\n","hidden4/kernel/Initializer/random_uniform/RandomUniform\n","hidden4/kernel/Initializer/random_uniform/sub\n","hidden4/kernel/Initializer/random_uniform/mul\n","hidden4/kernel/Initializer/random_uniform\n","hidden4/kernel\n","hidden4/kernel/Assign\n","hidden4/kernel/read\n","hidden4/bias/Initializer/zeros\n","hidden4/bias\n","hidden4/bias/Assign\n","hidden4/bias/read\n","dnn/hidden4/MatMul\n","dnn/hidden4/BiasAdd\n","dnn/hidden4/Relu\n","hidden5/kernel/Initializer/random_uniform/shape\n","hidden5/kernel/Initializer/random_uniform/min\n","hidden5/kernel/Initializer/random_uniform/max\n","hidden5/kernel/Initializer/random_uniform/RandomUniform\n","hidden5/kernel/Initializer/random_uniform/sub\n","hidden5/kernel/Initializer/random_uniform/mul\n","hidden5/kernel/Initializer/random_uniform\n","hidden5/kernel\n","hidden5/kernel/Assign\n","hidden5/kernel/read\n","hidden5/bias/Initializer/zeros\n","hidden5/bias\n","hidden5/bias/Assign\n","hidden5/bias/read\n","dnn/hidden5/MatMul\n","dnn/hidden5/BiasAdd\n","dnn/hidden5/Relu\n","outputs/kernel/Initializer/random_uniform/shape\n","outputs/kernel/Initializer/random_uniform/min\n","outputs/kernel/Initializer/random_uniform/max\n","outputs/kernel/Initializer/random_uniform/RandomUniform\n","outputs/kernel/Initializer/random_uniform/sub\n","outputs/kernel/Initializer/random_uniform/mul\n","outputs/kernel/Initializer/random_uniform\n","outputs/kernel\n","outputs/kernel/Assign\n","outputs/kernel/read\n","outputs/bias/Initializer/zeros\n","outputs/bias\n","outputs/bias/Assign\n","outputs/bias/read\n","dnn/outputs/MatMul\n","dnn/outputs/BiasAdd\n","loss/SparseSoftmaxCrossEntropyWithLogits/Shape\n","loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n","loss/Const\n","loss/loss\n","gradients/Shape\n","gradients/grad_ys_0\n","gradients/Fill\n","gradients/loss/loss_grad/Reshape/shape\n","gradients/loss/loss_grad/Reshape\n","gradients/loss/loss_grad/Shape\n","gradients/loss/loss_grad/Tile\n","gradients/loss/loss_grad/Shape_1\n","gradients/loss/loss_grad/Shape_2\n","gradients/loss/loss_grad/Const\n","gradients/loss/loss_grad/Prod\n","gradients/loss/loss_grad/Const_1\n","gradients/loss/loss_grad/Prod_1\n","gradients/loss/loss_grad/Maximum/y\n","gradients/loss/loss_grad/Maximum\n","gradients/loss/loss_grad/floordiv\n","gradients/loss/loss_grad/Cast\n","gradients/loss/loss_grad/truediv\n","gradients/zeros_like\n","gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient\n","gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim\n","gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims\n","gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul\n","gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad\n","gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps\n","gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency\n","gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1\n","gradients/dnn/outputs/MatMul_grad/MatMul\n","gradients/dnn/outputs/MatMul_grad/MatMul_1\n","gradients/dnn/outputs/MatMul_grad/tuple/group_deps\n","gradients/dnn/outputs/MatMul_grad/tuple/control_dependency\n","gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1\n","gradients/dnn/hidden5/Relu_grad/ReluGrad\n","gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad\n","gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps\n","gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency\n","gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1\n","gradients/dnn/hidden5/MatMul_grad/MatMul\n","gradients/dnn/hidden5/MatMul_grad/MatMul_1\n","gradients/dnn/hidden5/MatMul_grad/tuple/group_deps\n","gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency\n","gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1\n","gradients/dnn/hidden4/Relu_grad/ReluGrad\n","gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad\n","gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps\n","gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency\n","gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1\n","gradients/dnn/hidden4/MatMul_grad/MatMul\n","gradients/dnn/hidden4/MatMul_grad/MatMul_1\n","gradients/dnn/hidden4/MatMul_grad/tuple/group_deps\n","gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency\n","gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1\n","gradients/dnn/hidden3/Relu_grad/ReluGrad\n","gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad\n","gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps\n","gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency\n","gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1\n","gradients/dnn/hidden3/MatMul_grad/MatMul\n","gradients/dnn/hidden3/MatMul_grad/MatMul_1\n","gradients/dnn/hidden3/MatMul_grad/tuple/group_deps\n","gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency\n","gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1\n","gradients/dnn/hidden2/Relu_grad/ReluGrad\n","gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad\n","gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps\n","gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency\n","gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1\n","gradients/dnn/hidden2/MatMul_grad/MatMul\n","gradients/dnn/hidden2/MatMul_grad/MatMul_1\n","gradients/dnn/hidden2/MatMul_grad/tuple/group_deps\n","gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency\n","gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1\n","gradients/dnn/hidden1/Relu_grad/ReluGrad\n","gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad\n","gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps\n","gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency\n","gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1\n","gradients/dnn/hidden1/MatMul_grad/MatMul\n","gradients/dnn/hidden1/MatMul_grad/MatMul_1\n","gradients/dnn/hidden1/MatMul_grad/tuple/group_deps\n","gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency\n","gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1\n","clip_by_value/Minimum/y\n","clip_by_value/Minimum\n","clip_by_value/y\n","clip_by_value\n","clip_by_value_1/Minimum/y\n","clip_by_value_1/Minimum\n","clip_by_value_1/y\n","clip_by_value_1\n","clip_by_value_2/Minimum/y\n","clip_by_value_2/Minimum\n","clip_by_value_2/y\n","clip_by_value_2\n","clip_by_value_3/Minimum/y\n","clip_by_value_3/Minimum\n","clip_by_value_3/y\n","clip_by_value_3\n","clip_by_value_4/Minimum/y\n","clip_by_value_4/Minimum\n","clip_by_value_4/y\n","clip_by_value_4\n","clip_by_value_5/Minimum/y\n","clip_by_value_5/Minimum\n","clip_by_value_5/y\n","clip_by_value_5\n","clip_by_value_6/Minimum/y\n","clip_by_value_6/Minimum\n","clip_by_value_6/y\n","clip_by_value_6\n","clip_by_value_7/Minimum/y\n","clip_by_value_7/Minimum\n","clip_by_value_7/y\n","clip_by_value_7\n","clip_by_value_8/Minimum/y\n","clip_by_value_8/Minimum\n","clip_by_value_8/y\n","clip_by_value_8\n","clip_by_value_9/Minimum/y\n","clip_by_value_9/Minimum\n","clip_by_value_9/y\n","clip_by_value_9\n","clip_by_value_10/Minimum/y\n","clip_by_value_10/Minimum\n","clip_by_value_10/y\n","clip_by_value_10\n","clip_by_value_11/Minimum/y\n","clip_by_value_11/Minimum\n","clip_by_value_11/y\n","clip_by_value_11\n","GradientDescent/learning_rate\n","GradientDescent/update_hidden1/kernel/ApplyGradientDescent\n","GradientDescent/update_hidden1/bias/ApplyGradientDescent\n","GradientDescent/update_hidden2/kernel/ApplyGradientDescent\n","GradientDescent/update_hidden2/bias/ApplyGradientDescent\n","GradientDescent/update_hidden3/kernel/ApplyGradientDescent\n","GradientDescent/update_hidden3/bias/ApplyGradientDescent\n","GradientDescent/update_hidden4/kernel/ApplyGradientDescent\n","GradientDescent/update_hidden4/bias/ApplyGradientDescent\n","GradientDescent/update_hidden5/kernel/ApplyGradientDescent\n","GradientDescent/update_hidden5/bias/ApplyGradientDescent\n","GradientDescent/update_outputs/kernel/ApplyGradientDescent\n","GradientDescent/update_outputs/bias/ApplyGradientDescent\n","GradientDescent\n","gradients_1/Shape\n","gradients_1/grad_ys_0\n","gradients_1/Fill\n","gradients_1/loss/loss_grad/Reshape/shape\n","gradients_1/loss/loss_grad/Reshape\n","gradients_1/loss/loss_grad/Shape\n","gradients_1/loss/loss_grad/Tile\n","gradients_1/loss/loss_grad/Shape_1\n","gradients_1/loss/loss_grad/Shape_2\n","gradients_1/loss/loss_grad/Const\n","gradients_1/loss/loss_grad/Prod\n","gradients_1/loss/loss_grad/Const_1\n","gradients_1/loss/loss_grad/Prod_1\n","gradients_1/loss/loss_grad/Maximum/y\n","gradients_1/loss/loss_grad/Maximum\n","gradients_1/loss/loss_grad/floordiv\n","gradients_1/loss/loss_grad/Cast\n","gradients_1/loss/loss_grad/truediv\n","gradients_1/zeros_like\n","gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient\n","gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim\n","gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims\n","gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul\n","gradients_1/dnn/outputs/BiasAdd_grad/BiasAddGrad\n","gradients_1/dnn/outputs/BiasAdd_grad/tuple/group_deps\n","gradients_1/dnn/outputs/BiasAdd_grad/tuple/control_dependency\n","gradients_1/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1\n","gradients_1/dnn/outputs/MatMul_grad/MatMul\n","gradients_1/dnn/outputs/MatMul_grad/MatMul_1\n","gradients_1/dnn/outputs/MatMul_grad/tuple/group_deps\n","gradients_1/dnn/outputs/MatMul_grad/tuple/control_dependency\n","gradients_1/dnn/outputs/MatMul_grad/tuple/control_dependency_1\n","gradients_1/dnn/hidden5/Relu_grad/ReluGrad\n","gradients_1/dnn/hidden5/BiasAdd_grad/BiasAddGrad\n","gradients_1/dnn/hidden5/BiasAdd_grad/tuple/group_deps\n","gradients_1/dnn/hidden5/BiasAdd_grad/tuple/control_dependency\n","gradients_1/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1\n","gradients_1/dnn/hidden5/MatMul_grad/MatMul\n","gradients_1/dnn/hidden5/MatMul_grad/MatMul_1\n","gradients_1/dnn/hidden5/MatMul_grad/tuple/group_deps\n","gradients_1/dnn/hidden5/MatMul_grad/tuple/control_dependency\n","gradients_1/dnn/hidden5/MatMul_grad/tuple/control_dependency_1\n","gradients_1/dnn/hidden4/Relu_grad/ReluGrad\n","gradients_1/dnn/hidden4/BiasAdd_grad/BiasAddGrad\n","gradients_1/dnn/hidden4/BiasAdd_grad/tuple/group_deps\n","gradients_1/dnn/hidden4/BiasAdd_grad/tuple/control_dependency\n","gradients_1/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1\n","gradients_1/dnn/hidden4/MatMul_grad/MatMul\n","gradients_1/dnn/hidden4/MatMul_grad/MatMul_1\n","gradients_1/dnn/hidden4/MatMul_grad/tuple/group_deps\n","gradients_1/dnn/hidden4/MatMul_grad/tuple/control_dependency\n","gradients_1/dnn/hidden4/MatMul_grad/tuple/control_dependency_1\n","gradients_1/dnn/hidden3/Relu_grad/ReluGrad\n","gradients_1/dnn/hidden3/BiasAdd_grad/BiasAddGrad\n","gradients_1/dnn/hidden3/BiasAdd_grad/tuple/group_deps\n","gradients_1/dnn/hidden3/BiasAdd_grad/tuple/control_dependency\n","gradients_1/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1\n","gradients_1/dnn/hidden3/MatMul_grad/MatMul\n","gradients_1/dnn/hidden3/MatMul_grad/MatMul_1\n","gradients_1/dnn/hidden3/MatMul_grad/tuple/group_deps\n","gradients_1/dnn/hidden3/MatMul_grad/tuple/control_dependency\n","gradients_1/dnn/hidden3/MatMul_grad/tuple/control_dependency_1\n","gradients_1/dnn/hidden2/Relu_grad/ReluGrad\n","gradients_1/dnn/hidden2/BiasAdd_grad/BiasAddGrad\n","gradients_1/dnn/hidden2/BiasAdd_grad/tuple/group_deps\n","gradients_1/dnn/hidden2/BiasAdd_grad/tuple/control_dependency\n","gradients_1/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1\n","gradients_1/dnn/hidden2/MatMul_grad/MatMul\n","gradients_1/dnn/hidden2/MatMul_grad/MatMul_1\n","gradients_1/dnn/hidden2/MatMul_grad/tuple/group_deps\n","gradients_1/dnn/hidden2/MatMul_grad/tuple/control_dependency\n","gradients_1/dnn/hidden2/MatMul_grad/tuple/control_dependency_1\n","gradients_1/dnn/hidden1/Relu_grad/ReluGrad\n","gradients_1/dnn/hidden1/BiasAdd_grad/BiasAddGrad\n","gradients_1/dnn/hidden1/BiasAdd_grad/tuple/group_deps\n","gradients_1/dnn/hidden1/BiasAdd_grad/tuple/control_dependency\n","gradients_1/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1\n","gradients_1/dnn/hidden1/MatMul_grad/MatMul\n","gradients_1/dnn/hidden1/MatMul_grad/MatMul_1\n","gradients_1/dnn/hidden1/MatMul_grad/tuple/group_deps\n","gradients_1/dnn/hidden1/MatMul_grad/tuple/control_dependency\n","gradients_1/dnn/hidden1/MatMul_grad/tuple/control_dependency_1\n","clip_by_value_12/Minimum/y\n","clip_by_value_12/Minimum\n","clip_by_value_12/y\n","clip_by_value_12\n","clip_by_value_13/Minimum/y\n","clip_by_value_13/Minimum\n","clip_by_value_13/y\n","clip_by_value_13\n","clip_by_value_14/Minimum/y\n","clip_by_value_14/Minimum\n","clip_by_value_14/y\n","clip_by_value_14\n","clip_by_value_15/Minimum/y\n","clip_by_value_15/Minimum\n","clip_by_value_15/y\n","clip_by_value_15\n","clip_by_value_16/Minimum/y\n","clip_by_value_16/Minimum\n","clip_by_value_16/y\n","clip_by_value_16\n","clip_by_value_17/Minimum/y\n","clip_by_value_17/Minimum\n","clip_by_value_17/y\n","clip_by_value_17\n","clip_by_value_18/Minimum/y\n","clip_by_value_18/Minimum\n","clip_by_value_18/y\n","clip_by_value_18\n","clip_by_value_19/Minimum/y\n","clip_by_value_19/Minimum\n","clip_by_value_19/y\n","clip_by_value_19\n","clip_by_value_20/Minimum/y\n","clip_by_value_20/Minimum\n","clip_by_value_20/y\n","clip_by_value_20\n","clip_by_value_21/Minimum/y\n","clip_by_value_21/Minimum\n","clip_by_value_21/y\n","clip_by_value_21\n","clip_by_value_22/Minimum/y\n","clip_by_value_22/Minimum\n","clip_by_value_22/y\n","clip_by_value_22\n","clip_by_value_23/Minimum/y\n","clip_by_value_23/Minimum\n","clip_by_value_23/y\n","clip_by_value_23\n","GradientDescent_1/learning_rate\n","GradientDescent_1/update_hidden1/kernel/ApplyGradientDescent\n","GradientDescent_1/update_hidden1/bias/ApplyGradientDescent\n","GradientDescent_1/update_hidden2/kernel/ApplyGradientDescent\n","GradientDescent_1/update_hidden2/bias/ApplyGradientDescent\n","GradientDescent_1/update_hidden3/kernel/ApplyGradientDescent\n","GradientDescent_1/update_hidden3/bias/ApplyGradientDescent\n","GradientDescent_1/update_hidden4/kernel/ApplyGradientDescent\n","GradientDescent_1/update_hidden4/bias/ApplyGradientDescent\n","GradientDescent_1/update_hidden5/kernel/ApplyGradientDescent\n","GradientDescent_1/update_hidden5/bias/ApplyGradientDescent\n","GradientDescent_1/update_outputs/kernel/ApplyGradientDescent\n","GradientDescent_1/update_outputs/bias/ApplyGradientDescent\n","GradientDescent_1\n","eval/in_top_k/InTopKV2/k\n","eval/in_top_k/InTopKV2\n","eval/Cast\n","eval/Const\n","eval/accuracy\n","init\n","save/filename/input\n","save/filename\n","save/Const\n","save/SaveV2/tensor_names\n","save/SaveV2/shape_and_slices\n","save/SaveV2\n","save/control_dependency\n","save/RestoreV2/tensor_names\n","save/RestoreV2/shape_and_slices\n","save/RestoreV2\n","save/Assign\n","save/Assign_1\n","save/Assign_2\n","save/Assign_3\n","save/Assign_4\n","save/Assign_5\n","save/Assign_6\n","save/Assign_7\n","save/Assign_8\n","save/Assign_9\n","save/Assign_10\n","save/Assign_11\n","save/restore_all\n"],"name":"stdout"}]},{"metadata":{"id":"Yn75qIEwGDTx","colab_type":"text"},"cell_type":"markdown","source":["Oops, that's a lot of operations! It's much easier to use TensorBoard to visualize the graph. The following hack will allow you to visualize the graph within Jupyter (if it does not work with your browser, you will need to use a `FileWriter` to save the graph and then visualize it in TensorBoard):"]},{"metadata":{"id":"SLJE-R3fGDTy","colab_type":"code","colab":{}},"cell_type":"code","source":["from tensorflow_graph_in_jupyter import show_graph"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":true,"id":"MWXsu7r2GDT0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":662},"outputId":"14f51fad-fb5c-4e09-8126-0069938ad62a","executionInfo":{"status":"ok","timestamp":1556500289333,"user_tz":240,"elapsed":520,"user":{"displayName":"Ruikang Wang","photoUrl":"https://lh6.googleusercontent.com/-sxpbEWf-1Ng/AAAAAAAAAAI/AAAAAAAAAJ4/3m9qUw3an_4/s64/photo.jpg","userId":"10914323651690992949"}}},"cell_type":"code","source":["show_graph(tf.get_default_graph())"],"execution_count":69,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n","        <script src=&quot;//cdnjs.cloudflare.com/ajax/libs/polymer/0.3.3/platform.js&quot;></script>\n","        <script>\n","          function load() {\n","            document.getElementById(&quot;graph0.3745401188473625&quot;).pbtxt = 'node {\\n  name: &quot;X&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n        dim {\\n          size: 784\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;y&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        unknown_rank: true\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\020\\\\003\\\\000\\\\000,\\\\001\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.07439795136451721\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.07439795136451721\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 784\\n        }\\n        dim {\\n          size: 300\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 300\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 300\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;hidden1/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden1/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;hidden1/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden1/MatMul&quot;\\n  input: &quot;hidden1/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden1/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;,\\\\001\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.13093073666095734\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.13093073666095734\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 22\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 300\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;hidden2/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden2/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden1/Relu&quot;\\n  input: &quot;hidden2/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden2/MatMul&quot;\\n  input: &quot;hidden2/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden2/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 39\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;hidden3/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden3/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden3/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden2/Relu&quot;\\n  input: &quot;hidden3/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden3/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden3/MatMul&quot;\\n  input: &quot;hidden3/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden3/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden3/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 56\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;hidden4/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden4/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden4/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden3/Relu&quot;\\n  input: &quot;hidden4/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden4/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden4/MatMul&quot;\\n  input: &quot;hidden4/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden4/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden4/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 73\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;hidden5/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden5/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden5/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden4/Relu&quot;\\n  input: &quot;hidden5/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden5/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden5/MatMul&quot;\\n  input: &quot;hidden5/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden5/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden5/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\000\\\\n\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.3162277638912201\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.3162277638912201\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 90\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;outputs/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 10\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;outputs/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;outputs/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/outputs/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden5/Relu&quot;\\n  input: &quot;outputs/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/outputs/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/outputs/MatMul&quot;\\n  input: &quot;outputs/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  op: &quot;SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;dnn/outputs/BiasAdd&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tlabels&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/loss&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;loss/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/grad_ys_0&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Shape&quot;\\n  input: &quot;gradients/grad_ys_0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;index_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Fill&quot;\\n  input: &quot;gradients/loss/loss_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/loss/loss_grad/Reshape&quot;\\n  input: &quot;gradients/loss/loss_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Shape_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Prod&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/loss/loss_grad/Shape_1&quot;\\n  input: &quot;gradients/loss/loss_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Prod_1&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/loss/loss_grad/Shape_2&quot;\\n  input: &quot;gradients/loss/loss_grad/Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/loss/loss_grad/Prod_1&quot;\\n  input: &quot;gradients/loss/loss_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/loss/loss_grad/Prod&quot;\\n  input: &quot;gradients/loss/loss_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;gradients/loss/loss_grad/floordiv&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Truncate&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/truediv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients/loss/loss_grad/Tile&quot;\\n  input: &quot;gradients/loss/loss_grad/Cast&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/zeros_like&quot;\\n  op: &quot;ZerosLike&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient&quot;\\n  op: &quot;PreventGradient&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;message&quot;\\n    value {\\n      s: &quot;Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation\\\\\\'s interaction with tf.gradients()&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: -1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;gradients/loss/loss_grad/truediv&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  input: &quot;^gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;outputs/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden5/Relu&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden5/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden5/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden4/Relu&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden4/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden4/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden3/Relu&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden3/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden3/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden2/Relu&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden2/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden2/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden1/Relu&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden1/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden1/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value/Minimum&quot;\\n  input: &quot;clip_by_value/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_1/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_1/Minimum&quot;\\n  input: &quot;clip_by_value_1/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_2/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_2/Minimum&quot;\\n  input: &quot;clip_by_value_2/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_3/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_3/Minimum&quot;\\n  input: &quot;clip_by_value_3/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_4/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_4/Minimum&quot;\\n  input: &quot;clip_by_value_4/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_5/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_5/Minimum&quot;\\n  input: &quot;clip_by_value_5/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_6/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_6/Minimum&quot;\\n  input: &quot;clip_by_value_6/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_7/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_7/Minimum&quot;\\n  input: &quot;clip_by_value_7/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_8/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_8/Minimum&quot;\\n  input: &quot;clip_by_value_8/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_9/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_9/Minimum&quot;\\n  input: &quot;clip_by_value_9/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_10/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_10/Minimum&quot;\\n  input: &quot;clip_by_value_10/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_11/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_11/Minimum&quot;\\n  input: &quot;clip_by_value_11/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.009999999776482582\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden1/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden1/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden2/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden2/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden3/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_4&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden3/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_5&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden4/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_6&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden4/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_7&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden5/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_8&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden5/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_9&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_outputs/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_10&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_outputs/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_11&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^GradientDescent/update_hidden1/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden1/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden2/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden2/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden3/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden3/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden4/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden4/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden5/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden5/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_outputs/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_outputs/kernel/ApplyGradientDescent&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/grad_ys_0&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients_1/Shape&quot;\\n  input: &quot;gradients_1/grad_ys_0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;index_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/loss_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/loss_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients_1/Fill&quot;\\n  input: &quot;gradients_1/loss/loss_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/loss_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/loss_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients_1/loss/loss_grad/Reshape&quot;\\n  input: &quot;gradients_1/loss/loss_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/loss_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/loss_grad/Shape_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/loss_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/loss_grad/Prod&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients_1/loss/loss_grad/Shape_1&quot;\\n  input: &quot;gradients_1/loss/loss_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/loss_grad/Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/loss_grad/Prod_1&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients_1/loss/loss_grad/Shape_2&quot;\\n  input: &quot;gradients_1/loss/loss_grad/Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/loss_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/loss_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients_1/loss/loss_grad/Prod_1&quot;\\n  input: &quot;gradients_1/loss/loss_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/loss_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients_1/loss/loss_grad/Prod&quot;\\n  input: &quot;gradients_1/loss/loss_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/loss_grad/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;gradients_1/loss/loss_grad/floordiv&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Truncate&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/loss_grad/truediv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients_1/loss/loss_grad/Tile&quot;\\n  input: &quot;gradients_1/loss/loss_grad/Cast&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/zeros_like&quot;\\n  op: &quot;ZerosLike&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient&quot;\\n  op: &quot;PreventGradient&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;message&quot;\\n    value {\\n      s: &quot;Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation\\\\\\'s interaction with tf.gradients()&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: -1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;gradients_1/loss/loss_grad/truediv&quot;\\n  input: &quot;gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims&quot;\\n  input: &quot;gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  input: &quot;^gradients_1/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients_1/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/outputs/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients_1/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;outputs/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden5/Relu&quot;\\n  input: &quot;gradients_1/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/dnn/outputs/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_1/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/outputs/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/outputs/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_1/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/outputs/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/outputs/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients_1/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients_1/dnn/outputs/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden5/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients_1/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients_1/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients_1/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients_1/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden5/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients_1/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden5/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden4/Relu&quot;\\n  input: &quot;gradients_1/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/dnn/hidden5/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_1/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden5/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden5/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_1/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden5/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden5/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients_1/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients_1/dnn/hidden5/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden4/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients_1/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients_1/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients_1/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients_1/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients_1/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden4/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden3/Relu&quot;\\n  input: &quot;gradients_1/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_1/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden4/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_1/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden4/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden4/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients_1/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients_1/dnn/hidden4/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden3/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients_1/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients_1/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients_1/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients_1/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients_1/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden3/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden2/Relu&quot;\\n  input: &quot;gradients_1/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_1/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden3/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_1/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden3/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden3/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients_1/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients_1/dnn/hidden3/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden2/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients_1/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients_1/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients_1/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients_1/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients_1/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden2/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden1/Relu&quot;\\n  input: &quot;gradients_1/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_1/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden2/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_1/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden2/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden2/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients_1/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients_1/dnn/hidden2/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden1/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients_1/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients_1/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients_1/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients_1/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients_1/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden1/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;gradients_1/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients_1/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_1/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden1/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients_1/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden1/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients_1/dnn/hidden1/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients_1/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients_1/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients_1/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_12/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_12/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients_1/dnn/hidden1/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_12/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_12/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_12&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_12/Minimum&quot;\\n  input: &quot;clip_by_value_12/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_13/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_13/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients_1/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_13/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_13/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_13&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_13/Minimum&quot;\\n  input: &quot;clip_by_value_13/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_14/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_14/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients_1/dnn/hidden2/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_14/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_14/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_14&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_14/Minimum&quot;\\n  input: &quot;clip_by_value_14/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_15/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_15/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients_1/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_15/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_15/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_15&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_15/Minimum&quot;\\n  input: &quot;clip_by_value_15/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_16/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_16/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients_1/dnn/hidden3/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_16/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_16/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_16&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_16/Minimum&quot;\\n  input: &quot;clip_by_value_16/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_17/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_17/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients_1/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_17/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_17/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_17&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_17/Minimum&quot;\\n  input: &quot;clip_by_value_17/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_18/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_18/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients_1/dnn/hidden4/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_18/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_18/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_18&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_18/Minimum&quot;\\n  input: &quot;clip_by_value_18/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_19/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_19/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients_1/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_19/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_19/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_19&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_19/Minimum&quot;\\n  input: &quot;clip_by_value_19/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_20/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_20/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients_1/dnn/hidden5/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_20/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_20/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_20&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_20/Minimum&quot;\\n  input: &quot;clip_by_value_20/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_21/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_21/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients_1/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_21/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_21/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_21&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_21/Minimum&quot;\\n  input: &quot;clip_by_value_21/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_22/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_22/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients_1/dnn/outputs/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_22/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_22/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_22&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_22/Minimum&quot;\\n  input: &quot;clip_by_value_22/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_23/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_23/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients_1/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_23/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_23/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_23&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_23/Minimum&quot;\\n  input: &quot;clip_by_value_23/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.009999999776482582\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1/update_hidden1/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;GradientDescent_1/learning_rate&quot;\\n  input: &quot;clip_by_value_12&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1/update_hidden1/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;GradientDescent_1/learning_rate&quot;\\n  input: &quot;clip_by_value_13&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1/update_hidden2/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;GradientDescent_1/learning_rate&quot;\\n  input: &quot;clip_by_value_14&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1/update_hidden2/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;GradientDescent_1/learning_rate&quot;\\n  input: &quot;clip_by_value_15&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1/update_hidden3/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;GradientDescent_1/learning_rate&quot;\\n  input: &quot;clip_by_value_16&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1/update_hidden3/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;GradientDescent_1/learning_rate&quot;\\n  input: &quot;clip_by_value_17&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1/update_hidden4/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;GradientDescent_1/learning_rate&quot;\\n  input: &quot;clip_by_value_18&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1/update_hidden4/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;GradientDescent_1/learning_rate&quot;\\n  input: &quot;clip_by_value_19&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1/update_hidden5/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;GradientDescent_1/learning_rate&quot;\\n  input: &quot;clip_by_value_20&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1/update_hidden5/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;GradientDescent_1/learning_rate&quot;\\n  input: &quot;clip_by_value_21&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1/update_outputs/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;GradientDescent_1/learning_rate&quot;\\n  input: &quot;clip_by_value_22&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1/update_outputs/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;GradientDescent_1/learning_rate&quot;\\n  input: &quot;clip_by_value_23&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent_1&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^GradientDescent_1/update_hidden1/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent_1/update_hidden1/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent_1/update_hidden2/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent_1/update_hidden2/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent_1/update_hidden3/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent_1/update_hidden3/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent_1/update_hidden4/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent_1/update_hidden4/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent_1/update_hidden5/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent_1/update_hidden5/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent_1/update_outputs/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent_1/update_outputs/kernel/ApplyGradientDescent&quot;\\n}\\nnode {\\n  name: &quot;eval/in_top_k/InTopKV2/k&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/in_top_k/InTopKV2&quot;\\n  op: &quot;InTopKV2&quot;\\n  input: &quot;dnn/outputs/BiasAdd&quot;\\n  input: &quot;y&quot;\\n  input: &quot;eval/in_top_k/InTopKV2/k&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;eval/in_top_k/InTopKV2&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n  attr {\\n    key: &quot;Truncate&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/accuracy&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;eval/Cast&quot;\\n  input: &quot;eval/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^hidden1/bias/Assign&quot;\\n  input: &quot;^hidden1/kernel/Assign&quot;\\n  input: &quot;^hidden2/bias/Assign&quot;\\n  input: &quot;^hidden2/kernel/Assign&quot;\\n  input: &quot;^hidden3/bias/Assign&quot;\\n  input: &quot;^hidden3/kernel/Assign&quot;\\n  input: &quot;^hidden4/bias/Assign&quot;\\n  input: &quot;^hidden4/kernel/Assign&quot;\\n  input: &quot;^hidden5/bias/Assign&quot;\\n  input: &quot;^hidden5/kernel/Assign&quot;\\n  input: &quot;^outputs/bias/Assign&quot;\\n  input: &quot;^outputs/kernel/Assign&quot;\\n}\\nnode {\\n  name: &quot;save/filename/input&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;model&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/filename&quot;\\n  op: &quot;PlaceholderWithDefault&quot;\\n  input: &quot;save/filename/input&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Const&quot;\\n  op: &quot;PlaceholderWithDefault&quot;\\n  input: &quot;save/filename&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        string_val: &quot;hidden1/bias&quot;\\n        string_val: &quot;hidden1/kernel&quot;\\n        string_val: &quot;hidden2/bias&quot;\\n        string_val: &quot;hidden2/kernel&quot;\\n        string_val: &quot;hidden3/bias&quot;\\n        string_val: &quot;hidden3/kernel&quot;\\n        string_val: &quot;hidden4/bias&quot;\\n        string_val: &quot;hidden4/kernel&quot;\\n        string_val: &quot;hidden5/bias&quot;\\n        string_val: &quot;hidden5/kernel&quot;\\n        string_val: &quot;outputs/bias&quot;\\n        string_val: &quot;outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2&quot;\\n  op: &quot;SaveV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/SaveV2/tensor_names&quot;\\n  input: &quot;save/SaveV2/shape_and_slices&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;outputs/kernel&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;^save/SaveV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@save/Const&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        string_val: &quot;hidden1/bias&quot;\\n        string_val: &quot;hidden1/kernel&quot;\\n        string_val: &quot;hidden2/bias&quot;\\n        string_val: &quot;hidden2/kernel&quot;\\n        string_val: &quot;hidden3/bias&quot;\\n        string_val: &quot;hidden3/kernel&quot;\\n        string_val: &quot;hidden4/bias&quot;\\n        string_val: &quot;hidden4/kernel&quot;\\n        string_val: &quot;hidden5/bias&quot;\\n        string_val: &quot;hidden5/kernel&quot;\\n        string_val: &quot;outputs/bias&quot;\\n        string_val: &quot;outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2/tensor_names&quot;\\n  input: &quot;save/RestoreV2/shape_and_slices&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;save/RestoreV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_1&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;save/RestoreV2:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_2&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;save/RestoreV2:2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_3&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;save/RestoreV2:3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_4&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;save/RestoreV2:4&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_5&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;save/RestoreV2:5&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_6&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;save/RestoreV2:6&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_7&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;save/RestoreV2:7&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_8&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;save/RestoreV2:8&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_9&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;save/RestoreV2:9&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_10&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;save/RestoreV2:10&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_11&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;save/RestoreV2:11&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/restore_all&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^save/Assign&quot;\\n  input: &quot;^save/Assign_1&quot;\\n  input: &quot;^save/Assign_10&quot;\\n  input: &quot;^save/Assign_11&quot;\\n  input: &quot;^save/Assign_2&quot;\\n  input: &quot;^save/Assign_3&quot;\\n  input: &quot;^save/Assign_4&quot;\\n  input: &quot;^save/Assign_5&quot;\\n  input: &quot;^save/Assign_6&quot;\\n  input: &quot;^save/Assign_7&quot;\\n  input: &quot;^save/Assign_8&quot;\\n  input: &quot;^save/Assign_9&quot;\\n}\\n';\n","          }\n","        </script>\n","        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n","        <div style=&quot;height:600px&quot;>\n","          <tf-graph-basic id=&quot;graph0.3745401188473625&quot;></tf-graph-basic>\n","        </div>\n","    \"></iframe>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"ia7zvvlpGDT4","colab_type":"text"},"cell_type":"markdown","source":["Once you know which operations you need, you can get a handle on them using the graph's `get_operation_by_name()` or `get_tensor_by_name()` methods:"]},{"metadata":{"id":"kRcH7F3WGDT7","colab_type":"code","colab":{}},"cell_type":"code","source":["X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n","y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n","\n","accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n","\n","training_op = tf.get_default_graph().get_operation_by_name(\"GradientDescent\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XteISKYUGDT-","colab_type":"text"},"cell_type":"markdown","source":["If you are the author of the original model, you could make things easier for people who will reuse your model by giving operations very clear names and documenting them. Another approach is to create a collection containing all the important operations that people will want to get a handle on:"]},{"metadata":{"id":"RNg5dshwGDT_","colab_type":"code","colab":{}},"cell_type":"code","source":["for op in (X, y, accuracy, training_op):\n","    tf.add_to_collection(\"my_important_ops\", op)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GEx9LPknGDUE","colab_type":"text"},"cell_type":"markdown","source":["This way people who reuse your model will be able to simply write:"]},{"metadata":{"id":"QIGsMml1GDUF","colab_type":"code","colab":{}},"cell_type":"code","source":["X, y, accuracy, training_op = tf.get_collection(\"my_important_ops\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"goF5tJ0nGDUJ","colab_type":"text"},"cell_type":"markdown","source":["Now you can start a session, restore the model's state and continue training on your data:"]},{"metadata":{"id":"vGGqYS1EGDUK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":106},"outputId":"02855b5a-1037-41e2-c6b7-d48c9e7ac745","executionInfo":{"status":"ok","timestamp":1556500307229,"user_tz":240,"elapsed":260,"user":{"displayName":"Ruikang Wang","photoUrl":"https://lh6.googleusercontent.com/-sxpbEWf-1Ng/AAAAAAAAAAI/AAAAAAAAAJ4/3m9qUw3an_4/s64/photo.jpg","userId":"10914323651690992949"}}},"cell_type":"code","source":["with tf.Session() as sess:\n","    saver.restore(sess, \"./my_model_final.ckpt\")\n","    # continue training the model..."],"execution_count":73,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n","INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"],"name":"stdout"}]},{"metadata":{"id":"fZjpIYIfGDUO","colab_type":"text"},"cell_type":"markdown","source":["Actually, let's test this for real!"]},{"metadata":{"id":"FaC48Tj_GDUP","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.Session() as sess:\n","    saver.restore(sess, \"./my_model_final.ckpt\")\n","\n","    for epoch in range(n_epochs):\n","        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n","        print(epoch, \"Validation accuracy:\", accuracy_val)\n","\n","    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"MyxF-ZzMGDUS","colab_type":"text"},"cell_type":"markdown","source":["Alternatively, if you have access to the Python code that built the original graph, you can use it instead of `import_meta_graph()`:"]},{"metadata":{"id":"RWuBAWxTGDUS","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 28 * 28  # MNIST\n","n_hidden1 = 300\n","n_hidden2 = 50\n","n_hidden3 = 50\n","n_hidden4 = 50\n","n_hidden5 = 50\n","n_outputs = 10\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n","\n","with tf.name_scope(\"dnn\"):\n","    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n","    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n","    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n","    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n","    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n","    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n","\n","with tf.name_scope(\"loss\"):\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","    loss = tf.reduce_mean(xentropy, name=\"loss\")\n","\n","with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n","\n","learning_rate = 0.01\n","threshold = 1.0\n","\n","optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","grads_and_vars = optimizer.compute_gradients(loss)\n","capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n","              for grad, var in grads_and_vars]\n","training_op = optimizer.apply_gradients(capped_gvs)\n","\n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZvZ8vg4RGDUX","colab_type":"text"},"cell_type":"markdown","source":["And continue training:"]},{"metadata":{"id":"fECeHKw_GDUZ","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.Session() as sess:\n","    saver.restore(sess, \"./my_model_final.ckpt\")\n","\n","    for epoch in range(n_epochs):\n","        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n","        print(epoch, \"Validation accuracy:\", accuracy_val)\n","\n","    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"LTwFvssxGDUd","colab_type":"text"},"cell_type":"markdown","source":["In general you will want to reuse only the lower layers. If you are using `import_meta_graph()` it will load the whole graph, but you can simply ignore the parts you do not need. In this example, we add a new 4th hidden layer on top of the pretrained 3rd layer (ignoring the old 4th hidden layer). We also build a new output layer, the loss for this new output, and a new optimizer to minimize it. We also need another saver to save the whole graph (containing both the entire old graph plus the new operations), and an initialization operation to initialize all the new variables:"]},{"metadata":{"id":"z6HvRUUtGDUf","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_hidden4 = 20  # new layer\n","n_outputs = 10  # new layer\n","\n","saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")\n","\n","X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n","y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n","\n","hidden3 = tf.get_default_graph().get_tensor_by_name(\"dnn/hidden3/Relu:0\")\n","\n","new_hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"new_hidden4\")\n","new_logits = tf.layers.dense(new_hidden4, n_outputs, name=\"new_outputs\")\n","\n","with tf.name_scope(\"new_loss\"):\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=new_logits)\n","    loss = tf.reduce_mean(xentropy, name=\"loss\")\n","\n","with tf.name_scope(\"new_eval\"):\n","    correct = tf.nn.in_top_k(new_logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n","\n","with tf.name_scope(\"new_train\"):\n","    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","    training_op = optimizer.minimize(loss)\n","\n","init = tf.global_variables_initializer()\n","new_saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UKi7vaj0GDUi","colab_type":"text"},"cell_type":"markdown","source":["And we can train this new model:"]},{"metadata":{"id":"SfgEH2DLGDUj","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.Session() as sess:\n","    init.run()\n","    saver.restore(sess, \"./my_model_final.ckpt\")\n","\n","    for epoch in range(n_epochs):\n","        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n","        print(epoch, \"Validation accuracy:\", accuracy_val)\n","\n","    save_path = new_saver.save(sess, \"./my_new_model_final.ckpt\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"C_JufCJTGDUl","colab_type":"text"},"cell_type":"markdown","source":["If you have access to the Python code that built the original graph, you can just reuse the parts you need and drop the rest:"]},{"metadata":{"id":"ojR9exM_GDUm","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 28 * 28  # MNIST\n","n_hidden1 = 300 # reused\n","n_hidden2 = 50  # reused\n","n_hidden3 = 50  # reused\n","n_hidden4 = 20  # new!\n","n_outputs = 10  # new!\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n","\n","with tf.name_scope(\"dnn\"):\n","    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # reused\n","    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused\n","    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused\n","    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new!\n","    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # new!\n","\n","with tf.name_scope(\"loss\"):\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","    loss = tf.reduce_mean(xentropy, name=\"loss\")\n","\n","with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n","\n","with tf.name_scope(\"train\"):\n","    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","    training_op = optimizer.minimize(loss)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"m5-KVyFiGDUo","colab_type":"text"},"cell_type":"markdown","source":["However, you must create one `Saver` to restore the pretrained model (giving it the list of variables to restore, or else it will complain that the graphs don't match), and another `Saver` to save the new model, once it is trained:"]},{"metadata":{"id":"huZgE8JuGDUr","colab_type":"code","colab":{}},"cell_type":"code","source":["reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n","                               scope=\"hidden[123]\") # regular expression\n","restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n","\n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()\n","\n","with tf.Session() as sess:\n","    init.run()\n","    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n","\n","    for epoch in range(n_epochs):                                            # not shown in the book\n","        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): # not shown\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})        # not shown\n","        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})     # not shown\n","        print(epoch, \"Validation accuracy:\", accuracy_val)                   # not shown\n","\n","    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_OJnJCPlGDUu","colab_type":"text"},"cell_type":"markdown","source":["## Reusing Models from Other Frameworks"]},{"metadata":{"id":"ZgPY3bk5GDUv","colab_type":"text"},"cell_type":"markdown","source":["In this example, for each variable we want to reuse, we find its initializer's assignment operation, and we get its second input, which corresponds to the initialization value. When we run the initializer, we replace the initialization values with the ones we want, using a `feed_dict`:"]},{"metadata":{"id":"3eEinXjpGDUw","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 2\n","n_hidden1 = 3"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pFG432lyGDU3","colab_type":"code","colab":{}},"cell_type":"code","source":["original_w = [[1., 2., 3.], [4., 5., 6.]] # Load the weights from the other framework\n","original_b = [7., 8., 9.]                 # Load the biases from the other framework\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n","# [...] Build the rest of the model\n","\n","# Get a handle on the assignment nodes for the hidden1 variables\n","graph = tf.get_default_graph()\n","assign_kernel = graph.get_operation_by_name(\"hidden1/kernel/Assign\")\n","assign_bias = graph.get_operation_by_name(\"hidden1/bias/Assign\")\n","init_kernel = assign_kernel.inputs[1]\n","init_bias = assign_bias.inputs[1]\n","\n","init = tf.global_variables_initializer()\n","\n","with tf.Session() as sess:\n","    sess.run(init, feed_dict={init_kernel: original_w, init_bias: original_b})\n","    # [...] Train the model on your new task\n","    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))  # not shown in the book"],"execution_count":0,"outputs":[]},{"metadata":{"id":"w02ryGkqGDU-","colab_type":"text"},"cell_type":"markdown","source":["Note: the weights variable created by the `tf.layers.dense()` function is called `\"kernel\"` (instead of `\"weights\"` when using the `tf.contrib.layers.fully_connected()`, as in the book), and the biases variable is called `bias` instead of `biases`."]},{"metadata":{"id":"Vv9n4JkwGDU_","colab_type":"text"},"cell_type":"markdown","source":["Another approach (initially used in the book) would be to create dedicated assignment nodes and dedicated placeholders. This is more verbose and less efficient, but you may find this more explicit:"]},{"metadata":{"id":"r00iYjawGDU_","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 2\n","n_hidden1 = 3\n","\n","original_w = [[1., 2., 3.], [4., 5., 6.]] # Load the weights from the other framework\n","original_b = [7., 8., 9.]                 # Load the biases from the other framework\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n","# [...] Build the rest of the model\n","\n","# Get a handle on the variables of layer hidden1\n","with tf.variable_scope(\"\", default_name=\"\", reuse=True):  # root scope\n","    hidden1_weights = tf.get_variable(\"hidden1/kernel\")\n","    hidden1_biases = tf.get_variable(\"hidden1/bias\")\n","\n","# Create dedicated placeholders and assignment nodes\n","original_weights = tf.placeholder(tf.float32, shape=(n_inputs, n_hidden1))\n","original_biases = tf.placeholder(tf.float32, shape=n_hidden1)\n","assign_hidden1_weights = tf.assign(hidden1_weights, original_weights)\n","assign_hidden1_biases = tf.assign(hidden1_biases, original_biases)\n","\n","init = tf.global_variables_initializer()\n","\n","with tf.Session() as sess:\n","    sess.run(init)\n","    sess.run(assign_hidden1_weights, feed_dict={original_weights: original_w})\n","    sess.run(assign_hidden1_biases, feed_dict={original_biases: original_b})\n","    # [...] Train the model on your new task\n","    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PosNFaU-GDVE","colab_type":"text"},"cell_type":"markdown","source":["Note that we could also get a handle on the variables using `get_collection()` and specifying the `scope`:"]},{"metadata":{"id":"3SLpH-lMGDVG","colab_type":"code","colab":{}},"cell_type":"code","source":["tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden1\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SiNCFe0WGDVJ","colab_type":"text"},"cell_type":"markdown","source":["Or we could use the graph's `get_tensor_by_name()` method:"]},{"metadata":{"id":"F-2l2XoFGDVK","colab_type":"code","colab":{}},"cell_type":"code","source":["tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zapBVDw3GDVS","colab_type":"code","colab":{}},"cell_type":"code","source":["tf.get_default_graph().get_tensor_by_name(\"hidden1/bias:0\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"N4OWNbFXGDVZ","colab_type":"text"},"cell_type":"markdown","source":["### Freezing the Lower Layers"]},{"metadata":{"id":"q9JwMKdEGDVa","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 28 * 28  # MNIST\n","n_hidden1 = 300 # reused\n","n_hidden2 = 50  # reused\n","n_hidden3 = 50  # reused\n","n_hidden4 = 20  # new!\n","n_outputs = 10  # new!\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n","\n","with tf.name_scope(\"dnn\"):\n","    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # reused\n","    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused\n","    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused\n","    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new!\n","    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # new!\n","\n","with tf.name_scope(\"loss\"):\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","    loss = tf.reduce_mean(xentropy, name=\"loss\")\n","\n","with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OR7AE31KGDVh","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope(\"train\"):                                         # not shown in the book\n","    optimizer = tf.train.GradientDescentOptimizer(learning_rate)     # not shown\n","    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n","                                   scope=\"hidden[34]|outputs\")\n","    training_op = optimizer.minimize(loss, var_list=train_vars)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LVXAy_C5GDVn","colab_type":"code","colab":{}},"cell_type":"code","source":["init = tf.global_variables_initializer()\n","new_saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LujMl4RCGDVx","colab_type":"code","colab":{}},"cell_type":"code","source":["reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n","                               scope=\"hidden[123]\") # regular expression\n","restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n","\n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()\n","\n","with tf.Session() as sess:\n","    init.run()\n","    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n","\n","    for epoch in range(n_epochs):\n","        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n","        print(epoch, \"Validation accuracy:\", accuracy_val)\n","\n","    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-UnlcGNjGDVz","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 28 * 28  # MNIST\n","n_hidden1 = 300 # reused\n","n_hidden2 = 50  # reused\n","n_hidden3 = 50  # reused\n","n_hidden4 = 20  # new!\n","n_outputs = 10  # new!\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EQmMa5toGDV2","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope(\"dnn\"):\n","    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n","                              name=\"hidden1\") # reused frozen\n","    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n","                              name=\"hidden2\") # reused frozen\n","    hidden2_stop = tf.stop_gradient(hidden2)\n","    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n","                              name=\"hidden3\") # reused, not frozen\n","    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n","                              name=\"hidden4\") # new!\n","    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # new!"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EhR2UPcHGDV4","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope(\"loss\"):\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","    loss = tf.reduce_mean(xentropy, name=\"loss\")\n","\n","with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n","\n","with tf.name_scope(\"train\"):\n","    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","    training_op = optimizer.minimize(loss)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"C1Uzae6CGDV6","colab_type":"text"},"cell_type":"markdown","source":["The training code is exactly the same as earlier:"]},{"metadata":{"id":"fe9d30S6GDV7","colab_type":"code","colab":{}},"cell_type":"code","source":["reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n","                               scope=\"hidden[123]\") # regular expression\n","restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n","\n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()\n","\n","with tf.Session() as sess:\n","    init.run()\n","    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n","\n","    for epoch in range(n_epochs):\n","        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n","        print(epoch, \"Validation accuracy:\", accuracy_val)\n","\n","    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FC4e65mIGDWA","colab_type":"text"},"cell_type":"markdown","source":["### Caching the Frozen Layers"]},{"metadata":{"id":"MdGe0UsJGDWC","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 28 * 28  # MNIST\n","n_hidden1 = 300 # reused\n","n_hidden2 = 50  # reused\n","n_hidden3 = 50  # reused\n","n_hidden4 = 20  # new!\n","n_outputs = 10  # new!\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n","\n","with tf.name_scope(\"dnn\"):\n","    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n","                              name=\"hidden1\") # reused frozen\n","    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n","                              name=\"hidden2\") # reused frozen & cached\n","    hidden2_stop = tf.stop_gradient(hidden2)\n","    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n","                              name=\"hidden3\") # reused, not frozen\n","    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n","                              name=\"hidden4\") # new!\n","    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # new!\n","\n","with tf.name_scope(\"loss\"):\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","    loss = tf.reduce_mean(xentropy, name=\"loss\")\n","\n","with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n","\n","with tf.name_scope(\"train\"):\n","    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","    training_op = optimizer.minimize(loss)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ghZbiGxVGDWE","colab_type":"code","colab":{}},"cell_type":"code","source":["reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n","                               scope=\"hidden[123]\") # regular expression\n","restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n","\n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"f3rZ0KoTGDWH","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","\n","n_batches = len(X_train) // batch_size\n","\n","with tf.Session() as sess:\n","    init.run()\n","    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n","    \n","    h2_cache = sess.run(hidden2, feed_dict={X: X_train})\n","    h2_cache_valid = sess.run(hidden2, feed_dict={X: X_valid}) # not shown in the book\n","\n","    for epoch in range(n_epochs):\n","        shuffled_idx = np.random.permutation(len(X_train))\n","        hidden2_batches = np.array_split(h2_cache[shuffled_idx], n_batches)\n","        y_batches = np.array_split(y_train[shuffled_idx], n_batches)\n","        for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):\n","            sess.run(training_op, feed_dict={hidden2:hidden2_batch, y:y_batch})\n","\n","        accuracy_val = accuracy.eval(feed_dict={hidden2: h2_cache_valid, # not shown\n","                                                y: y_valid})             # not shown\n","        print(epoch, \"Validation accuracy:\", accuracy_val)               # not shown\n","\n","    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4sMxtdcbGDWK","colab_type":"text"},"cell_type":"markdown","source":["# Faster Optimizers"]},{"metadata":{"id":"ptODkL0ZGDWK","colab_type":"text"},"cell_type":"markdown","source":["## Momentum optimization"]},{"metadata":{"id":"sQ-QretpGDWK","colab_type":"code","colab":{}},"cell_type":"code","source":["optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n","                                       momentum=0.9)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lsSReQVOGDWO","colab_type":"text"},"cell_type":"markdown","source":["## Nesterov Accelerated Gradient"]},{"metadata":{"id":"hd4EfY_xGDWQ","colab_type":"code","colab":{}},"cell_type":"code","source":["optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n","                                       momentum=0.9, use_nesterov=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"t3CmTJzTGDWV","colab_type":"text"},"cell_type":"markdown","source":["## AdaGrad"]},{"metadata":{"id":"_PiZ0vCIGDWX","colab_type":"code","colab":{}},"cell_type":"code","source":["optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HhH6Xo6CGDWZ","colab_type":"text"},"cell_type":"markdown","source":["## RMSProp"]},{"metadata":{"id":"p5nggk9FGDWZ","colab_type":"code","colab":{}},"cell_type":"code","source":["optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate,\n","                                      momentum=0.9, decay=0.9, epsilon=1e-10)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-WxQQAvxGDWb","colab_type":"text"},"cell_type":"markdown","source":["## Adam Optimization"]},{"metadata":{"id":"Q9SAxoKEGDWb","colab_type":"code","colab":{}},"cell_type":"code","source":["optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"C0gzFd6zGDWc","colab_type":"text"},"cell_type":"markdown","source":["## Learning Rate Scheduling"]},{"metadata":{"id":"m7WGZ3qJGDWc","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 28 * 28  # MNIST\n","n_hidden1 = 300\n","n_hidden2 = 50\n","n_outputs = 10\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n","\n","with tf.name_scope(\"dnn\"):\n","    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n","    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n","    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n","\n","with tf.name_scope(\"loss\"):\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","    loss = tf.reduce_mean(xentropy, name=\"loss\")\n","\n","with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9OxsJiygGDWd","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope(\"train\"):       # not shown in the book\n","    initial_learning_rate = 0.1\n","    decay_steps = 10000\n","    decay_rate = 1/10\n","    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n","    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n","                                               decay_steps, decay_rate)\n","    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n","    training_op = optimizer.minimize(loss, global_step=global_step)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MTfSy-YQGDWh","colab_type":"code","colab":{}},"cell_type":"code","source":["init = tf.global_variables_initializer()\n","saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"h2n59bhjGDWr","colab_type":"code","colab":{}},"cell_type":"code","source":["n_epochs = 5\n","batch_size = 50\n","\n","with tf.Session() as sess:\n","    init.run()\n","    for epoch in range(n_epochs):\n","        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n","        print(epoch, \"Validation accuracy:\", accuracy_val)\n","\n","    save_path = saver.save(sess, \"./my_model_final.ckpt\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ITvUDvqCGDWx","colab_type":"text"},"cell_type":"markdown","source":["# Avoiding Overfitting Through Regularization"]},{"metadata":{"id":"iSSxMCQCGDWx","colab_type":"text"},"cell_type":"markdown","source":["## $\\ell_1$ and $\\ell_2$ regularization"]},{"metadata":{"id":"jJMenxffGDWy","colab_type":"text"},"cell_type":"markdown","source":["Let's implement $\\ell_1$ regularization manually. First, we create the model, as usual (with just one hidden layer this time, for simplicity):"]},{"metadata":{"id":"Wz2AyHU-GDWy","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 28 * 28  # MNIST\n","n_hidden1 = 300\n","n_outputs = 10\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n","\n","with tf.name_scope(\"dnn\"):\n","    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n","    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pWjBTTgXGDW2","colab_type":"text"},"cell_type":"markdown","source":["Next, we get a handle on the layer weights, and we compute the total loss, which is equal to the sum of the usual cross entropy loss and the $\\ell_1$ loss (i.e., the absolute values of the weights):"]},{"metadata":{"id":"zQWHefcQGDW2","colab_type":"code","colab":{}},"cell_type":"code","source":["W1 = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n","W2 = tf.get_default_graph().get_tensor_by_name(\"outputs/kernel:0\")\n","\n","scale = 0.001 # l1 regularization hyperparameter\n","\n","with tf.name_scope(\"loss\"):\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n","                                                              logits=logits)\n","    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n","    reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2))\n","    loss = tf.add(base_loss, scale * reg_losses, name=\"loss\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MZ1xrR9PGDW4","colab_type":"text"},"cell_type":"markdown","source":["The rest is just as usual:"]},{"metadata":{"id":"YuWrbd8hGDW5","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n","\n","learning_rate = 0.01\n","\n","with tf.name_scope(\"train\"):\n","    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","    training_op = optimizer.minimize(loss)\n","\n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":true,"id":"zIhKde72GDW6","colab_type":"code","colab":{}},"cell_type":"code","source":["n_epochs = 20\n","batch_size = 200\n","\n","with tf.Session() as sess:\n","    init.run()\n","    for epoch in range(n_epochs):\n","        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n","        print(epoch, \"Validation accuracy:\", accuracy_val)\n","\n","    save_path = saver.save(sess, \"./my_model_final.ckpt\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mr7RAvEZGDW7","colab_type":"text"},"cell_type":"markdown","source":["Alternatively, we can pass a regularization function to the `tf.layers.dense()` function, which will use it to create operations that will compute the regularization loss, and it adds these operations to the collection of regularization losses. The beginning is the same as above:"]},{"metadata":{"id":"lMrnk0iGGDW7","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 28 * 28  # MNIST\n","n_hidden1 = 300\n","n_hidden2 = 50\n","n_outputs = 10\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jGA3trLmGDW8","colab_type":"text"},"cell_type":"markdown","source":["Next, we will use Python's `partial()` function to avoid repeating the same arguments over and over again. Note that we set the `kernel_regularizer` argument:"]},{"metadata":{"id":"jqcad7WYGDW8","colab_type":"code","colab":{}},"cell_type":"code","source":["scale = 0.001"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ep08fInmGDW-","colab_type":"code","colab":{}},"cell_type":"code","source":["my_dense_layer = partial(\n","    tf.layers.dense, activation=tf.nn.relu,\n","    kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n","\n","with tf.name_scope(\"dnn\"):\n","    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n","    hidden2 = my_dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n","    logits = my_dense_layer(hidden2, n_outputs, activation=None,\n","                            name=\"outputs\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"U3zm1EA2GDXC","colab_type":"text"},"cell_type":"markdown","source":["Next we must add the regularization losses to the base loss:"]},{"metadata":{"id":"WcH8Gp-SGDXC","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope(\"loss\"):                                     # not shown in the book\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(  # not shown\n","        labels=y, logits=logits)                                # not shown\n","    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")   # not shown\n","    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n","    loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"u1VT-bU8GDXD","colab_type":"text"},"cell_type":"markdown","source":["And the rest is the same as usual:"]},{"metadata":{"id":"KIRawRidGDXE","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n","\n","learning_rate = 0.01\n","\n","with tf.name_scope(\"train\"):\n","    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n","    training_op = optimizer.minimize(loss)\n","\n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":true,"id":"7adNmYpWGDXF","colab_type":"code","colab":{}},"cell_type":"code","source":["n_epochs = 20\n","batch_size = 200\n","\n","with tf.Session() as sess:\n","    init.run()\n","    for epoch in range(n_epochs):\n","        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n","        print(epoch, \"Validation accuracy:\", accuracy_val)\n","\n","    save_path = saver.save(sess, \"./my_model_final.ckpt\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FhkAmRK4GDXG","colab_type":"text"},"cell_type":"markdown","source":["## Dropout"]},{"metadata":{"id":"vcBmjCdJGDXH","colab_type":"text"},"cell_type":"markdown","source":["Note: the book uses `tf.contrib.layers.dropout()` rather than `tf.layers.dropout()` (which did not exist when this chapter was written). It is now preferable to use `tf.layers.dropout()`, because anything in the contrib module may change or be deleted without notice. The `tf.layers.dropout()` function is almost identical to the `tf.contrib.layers.dropout()` function, except for a few minor differences. Most importantly:\n","* you must specify the dropout rate (`rate`) rather than the keep probability (`keep_prob`), where `rate` is simply equal to `1 - keep_prob`,\n","* the `is_training` parameter is renamed to `training`."]},{"metadata":{"id":"RHdVyqBRGDXI","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9fWVWZqpGDXJ","colab_type":"code","colab":{}},"cell_type":"code","source":["training = tf.placeholder_with_default(False, shape=(), name='training')\n","\n","dropout_rate = 0.5  # == 1 - keep_prob\n","X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n","\n","with tf.name_scope(\"dnn\"):\n","    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu,\n","                              name=\"hidden1\")\n","    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n","    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu,\n","                              name=\"hidden2\")\n","    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n","    logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"m1M-bZ1qGDXK","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope(\"loss\"):\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","    loss = tf.reduce_mean(xentropy, name=\"loss\")\n","\n","with tf.name_scope(\"train\"):\n","    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n","    training_op = optimizer.minimize(loss)    \n","\n","with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n","    \n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":true,"id":"wSQXlSF6GDXL","colab_type":"code","colab":{}},"cell_type":"code","source":["n_epochs = 20\n","batch_size = 50\n","\n","with tf.Session() as sess:\n","    init.run()\n","    for epoch in range(n_epochs):\n","        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n","        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n","        print(epoch, \"Validation accuracy:\", accuracy_val)\n","\n","    save_path = saver.save(sess, \"./my_model_final.ckpt\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Im-lVd7gGDXO","colab_type":"text"},"cell_type":"markdown","source":["## Max norm"]},{"metadata":{"id":"qleXy1WCGDXO","colab_type":"text"},"cell_type":"markdown","source":["Let's go back to a plain and simple neural net for MNIST with just 2 hidden layers:"]},{"metadata":{"id":"WPp39Nq9GDXP","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 28 * 28\n","n_hidden1 = 300\n","n_hidden2 = 50\n","n_outputs = 10\n","\n","learning_rate = 0.01\n","momentum = 0.9\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n","\n","with tf.name_scope(\"dnn\"):\n","    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n","    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n","    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n","\n","with tf.name_scope(\"loss\"):\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","    loss = tf.reduce_mean(xentropy, name=\"loss\")\n","\n","with tf.name_scope(\"train\"):\n","    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n","    training_op = optimizer.minimize(loss)    \n","\n","with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YMb42y-nGDXQ","colab_type":"text"},"cell_type":"markdown","source":["Next, let's get a handle on the first hidden layer's weight and create an operation that will compute the clipped weights using the `clip_by_norm()` function. Then we create an assignment operation to assign the clipped weights to the weights variable:"]},{"metadata":{"id":"PxtuNYx2GDXR","colab_type":"code","colab":{}},"cell_type":"code","source":["threshold = 1.0\n","weights = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n","clipped_weights = tf.clip_by_norm(weights, clip_norm=threshold, axes=1)\n","clip_weights = tf.assign(weights, clipped_weights)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jUyTsAaVGDXS","colab_type":"text"},"cell_type":"markdown","source":["We can do this as well for the second hidden layer:"]},{"metadata":{"id":"YNEmI9k1GDXS","colab_type":"code","colab":{}},"cell_type":"code","source":["weights2 = tf.get_default_graph().get_tensor_by_name(\"hidden2/kernel:0\")\n","clipped_weights2 = tf.clip_by_norm(weights2, clip_norm=threshold, axes=1)\n","clip_weights2 = tf.assign(weights2, clipped_weights2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"p71xmRwPGDXU","colab_type":"text"},"cell_type":"markdown","source":["Let's add an initializer and a saver:"]},{"metadata":{"id":"SqBswhHdGDXU","colab_type":"code","colab":{}},"cell_type":"code","source":["init = tf.global_variables_initializer()\n","saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ErplkKWGGDXW","colab_type":"text"},"cell_type":"markdown","source":["And now we can train the model. It's pretty much as usual, except that right after running the `training_op`, we run the `clip_weights` and `clip_weights2` operations:"]},{"metadata":{"id":"oEM1k_YFGDXY","colab_type":"code","colab":{}},"cell_type":"code","source":["n_epochs = 20\n","batch_size = 50"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hrAJ6xjIGDXd","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.Session() as sess:                                              # not shown in the book\n","    init.run()                                                          # not shown\n","    for epoch in range(n_epochs):                                       # not shown\n","        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): # not shown\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","            clip_weights.eval()\n","            clip_weights2.eval()                                        # not shown\n","        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})   # not shown\n","        print(epoch, \"Validation accuracy:\", acc_valid)                 # not shown\n","\n","    save_path = saver.save(sess, \"./my_model_final.ckpt\")               # not shown"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1dum0N8eGDXf","colab_type":"text"},"cell_type":"markdown","source":["The implementation above is straightforward and it works fine, but it is a bit messy. A better approach is to define a `max_norm_regularizer()` function:"]},{"metadata":{"id":"sd4_91VIGDXf","colab_type":"code","colab":{}},"cell_type":"code","source":["def max_norm_regularizer(threshold, axes=1, name=\"max_norm\",\n","                         collection=\"max_norm\"):\n","    def max_norm(weights):\n","        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n","        clip_weights = tf.assign(weights, clipped, name=name)\n","        tf.add_to_collection(collection, clip_weights)\n","        return None # there is no regularization loss term\n","    return max_norm"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jQPnMMYOGDXj","colab_type":"text"},"cell_type":"markdown","source":["Then you can call this function to get a max norm regularizer (with the threshold you want). When you create a hidden layer, you can pass this regularizer to the `kernel_regularizer` argument:"]},{"metadata":{"id":"wvCh5dAeGDXk","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 28 * 28\n","n_hidden1 = 300\n","n_hidden2 = 50\n","n_outputs = 10\n","\n","learning_rate = 0.01\n","momentum = 0.9\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2GAqaE93GDXm","colab_type":"code","colab":{}},"cell_type":"code","source":["max_norm_reg = max_norm_regularizer(threshold=1.0)\n","\n","with tf.name_scope(\"dnn\"):\n","    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n","                              kernel_regularizer=max_norm_reg, name=\"hidden1\")\n","    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n","                              kernel_regularizer=max_norm_reg, name=\"hidden2\")\n","    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3xaZthD7GDXo","colab_type":"code","colab":{}},"cell_type":"code","source":["with tf.name_scope(\"loss\"):\n","    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","    loss = tf.reduce_mean(xentropy, name=\"loss\")\n","\n","with tf.name_scope(\"train\"):\n","    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n","    training_op = optimizer.minimize(loss)    \n","\n","with tf.name_scope(\"eval\"):\n","    correct = tf.nn.in_top_k(logits, y, 1)\n","    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n","\n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WeSKrqHSGDXu","colab_type":"text"},"cell_type":"markdown","source":["Training is as usual, except you must run the weights clipping operations after each training operation:"]},{"metadata":{"id":"D-hN26KSGDXu","colab_type":"code","colab":{}},"cell_type":"code","source":["n_epochs = 20\n","batch_size = 50"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":false,"id":"GBDl_vAwGDXx","colab_type":"code","colab":{}},"cell_type":"code","source":["clip_all_weights = tf.get_collection(\"max_norm\")\n","\n","with tf.Session() as sess:\n","    init.run()\n","    for epoch in range(n_epochs):\n","        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","            sess.run(clip_all_weights)\n","        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid}) # not shown\n","        print(epoch, \"Validation accuracy:\", acc_valid)               # not shown\n","\n","    save_path = saver.save(sess, \"./my_model_final.ckpt\")             # not shown"],"execution_count":0,"outputs":[]},{"metadata":{"collapsed":true,"id":"pBKur8wJGDX1","colab_type":"text"},"cell_type":"markdown","source":["# Exercise solutions"]},{"metadata":{"id":"KqMkHd2zGDX1","colab_type":"text"},"cell_type":"markdown","source":["## 1. to 7."]},{"metadata":{"id":"rItK-94FGDX2","colab_type":"text"},"cell_type":"markdown","source":["See appendix A."]},{"metadata":{"id":"w7JO6t59GDX3","colab_type":"text"},"cell_type":"markdown","source":["## 8. Deep Learning"]},{"metadata":{"id":"5BBeCIHNGDX3","colab_type":"text"},"cell_type":"markdown","source":["### 8.1."]},{"metadata":{"id":"tIZoHAU_GDX4","colab_type":"text"},"cell_type":"markdown","source":["_Exercise: Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function._"]},{"metadata":{"id":"osaiGogKGDX7","colab_type":"text"},"cell_type":"markdown","source":["We will need similar DNNs in the next exercises, so let's create a function to build this DNN:"]},{"metadata":{"id":"hDZA5vr3GDX7","colab_type":"code","colab":{}},"cell_type":"code","source":["he_init = tf.variance_scaling_initializer()\n","\n","def dnn(inputs, n_hidden_layers=5, n_neurons=100, name=None,\n","        activation=tf.nn.elu, initializer=he_init):\n","    with tf.variable_scope(name, \"dnn\"):\n","        for layer in range(n_hidden_layers):\n","            inputs = tf.layers.dense(inputs, n_neurons, activation=activation,\n","                                     kernel_initializer=initializer,\n","                                     name=\"hidden%d\" % (layer + 1))\n","        return inputs"],"execution_count":0,"outputs":[]},{"metadata":{"id":"e1PN3MO6GDYG","colab_type":"code","colab":{}},"cell_type":"code","source":["n_inputs = 28 * 28 # MNIST\n","n_outputs = 5\n","\n","reset_graph()\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n","\n","dnn_outputs = dnn(X)\n","\n","logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init, name=\"logits\")\n","Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_oIxu3oKGDYI","colab_type":"text"},"cell_type":"markdown","source":["### 8.2."]},{"metadata":{"id":"I-gU5MhNGDYI","colab_type":"text"},"cell_type":"markdown","source":["_Exercise: Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You will need a softmax output layer with five neurons, and as always make sure to save checkpoints at regular intervals and save the final model so you can reuse it later._"]},{"metadata":{"id":"uEpV1HEqGDYJ","colab_type":"text"},"cell_type":"markdown","source":["Let's complete the graph with the cost function, the training op, and all the other usual components:"]},{"metadata":{"id":"V13J13oMGDYK","colab_type":"code","colab":{}},"cell_type":"code","source":["learning_rate = 0.01\n","\n","xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","loss = tf.reduce_mean(xentropy, name=\"loss\")\n","\n","optimizer = tf.train.AdamOptimizer(learning_rate)\n","training_op = optimizer.minimize(loss, name=\"training_op\")\n","\n","correct = tf.nn.in_top_k(logits, y, 1)\n","accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n","\n","init = tf.global_variables_initializer()\n","saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-9ju1L9DGDYO","colab_type":"text"},"cell_type":"markdown","source":["Now let's create the training set, validation and test set (we need the validation set to implement early stopping):"]},{"metadata":{"id":"X9a9ORQLGDYO","colab_type":"code","colab":{}},"cell_type":"code","source":["X_train1 = X_train[y_train < 5]\n","y_train1 = y_train[y_train < 5]\n","X_valid1 = X_valid[y_valid < 5]\n","y_valid1 = y_valid[y_valid < 5]\n","X_test1 = X_test[y_test < 5]\n","y_test1 = y_test[y_test < 5]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vSkrB6yFGDYR","colab_type":"code","colab":{}},"cell_type":"code","source":["n_epochs = 1000\n","batch_size = 20\n","\n","max_checks_without_progress = 20\n","checks_without_progress = 0\n","best_loss = np.infty\n","\n","with tf.Session() as sess:\n","    init.run()\n","\n","    for epoch in range(n_epochs):\n","        rnd_idx = np.random.permutation(len(X_train1))\n","        for rnd_indices in np.array_split(rnd_idx, len(X_train1) // batch_size):\n","            X_batch, y_batch = X_train1[rnd_indices], y_train1[rnd_indices]\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid1, y: y_valid1})\n","        if loss_val < best_loss:\n","            save_path = saver.save(sess, \"./my_mnist_model_0_to_4.ckpt\")\n","            best_loss = loss_val\n","            checks_without_progress = 0\n","        else:\n","            checks_without_progress += 1\n","            if checks_without_progress > max_checks_without_progress:\n","                print(\"Early stopping!\")\n","                break\n","        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n","            epoch, loss_val, best_loss, acc_val * 100))\n","\n","with tf.Session() as sess:\n","    saver.restore(sess, \"./my_mnist_model_0_to_4.ckpt\")\n","    acc_test = accuracy.eval(feed_dict={X: X_test1, y: y_test1})\n","    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ptMh0fqKGDYa","colab_type":"text"},"cell_type":"markdown","source":["We get 98.05% accuracy on the test set. That's not too bad, but let's see if we can do better by tuning the hyperparameters."]},{"metadata":{"id":"kwGOC_MBGDYb","colab_type":"text"},"cell_type":"markdown","source":["### 8.3."]},{"metadata":{"id":"qe2kix3wGDYb","colab_type":"text"},"cell_type":"markdown","source":["_Exercise: Tune the hyperparameters using cross-validation and see what precision you can achieve._"]},{"metadata":{"id":"7FGs_K2ZGDYb","colab_type":"text"},"cell_type":"markdown","source":["Let's create a `DNNClassifier` class, compatible with Scikit-Learn's `RandomizedSearchCV` class, to perform hyperparameter tuning. Here are the key points of this implementation:\n","* the `__init__()` method (constructor) does nothing more than create instance variables for each of the hyperparameters.\n","* the `fit()` method creates the graph, starts a session and trains the model:\n","  * it calls the `_build_graph()` method to build the graph (much lile the graph we defined earlier). Once this method is done creating the graph, it saves all the important operations as instance variables for easy access by other methods.\n","  * the `_dnn()` method builds the hidden layers, just like the `dnn()` function above, but also with support for batch normalization and dropout (for the next exercises).\n","  * if the `fit()` method is given a validation set (`X_valid` and `y_valid`), then it implements early stopping. This implementation does not save the best model to disk, but rather to memory: it uses the `_get_model_params()` method to get all the graph's variables and their values, and the `_restore_model_params()` method to restore the variable values (of the best model found). This trick helps speed up training.\n","  * After the `fit()` method has finished training the model, it keeps the session open so that predictions can be made quickly, without having to save a model to disk and restore it for every prediction. You can close the session by calling the `close_session()` method.\n","* the `predict_proba()` method uses the trained model to predict the class probabilities.\n","* the `predict()` method calls `predict_proba()` and returns the class with the highest probability, for each instance."]},{"metadata":{"id":"AxQ36romGDYc","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.base import BaseEstimator, ClassifierMixin\n","from sklearn.exceptions import NotFittedError\n","\n","class DNNClassifier(BaseEstimator, ClassifierMixin):\n","    def __init__(self, n_hidden_layers=5, n_neurons=100, optimizer_class=tf.train.AdamOptimizer,\n","                 learning_rate=0.01, batch_size=20, activation=tf.nn.elu, initializer=he_init,\n","                 batch_norm_momentum=None, dropout_rate=None, random_state=None):\n","        \"\"\"Initialize the DNNClassifier by simply storing all the hyperparameters.\"\"\"\n","        self.n_hidden_layers = n_hidden_layers\n","        self.n_neurons = n_neurons\n","        self.optimizer_class = optimizer_class\n","        self.learning_rate = learning_rate\n","        self.batch_size = batch_size\n","        self.activation = activation\n","        self.initializer = initializer\n","        self.batch_norm_momentum = batch_norm_momentum\n","        self.dropout_rate = dropout_rate\n","        self.random_state = random_state\n","        self._session = None\n","\n","    def _dnn(self, inputs):\n","        \"\"\"Build the hidden layers, with support for batch normalization and dropout.\"\"\"\n","        for layer in range(self.n_hidden_layers):\n","            if self.dropout_rate:\n","                inputs = tf.layers.dropout(inputs, self.dropout_rate, training=self._training)\n","            inputs = tf.layers.dense(inputs, self.n_neurons,\n","                                     kernel_initializer=self.initializer,\n","                                     name=\"hidden%d\" % (layer + 1))\n","            if self.batch_norm_momentum:\n","                inputs = tf.layers.batch_normalization(inputs, momentum=self.batch_norm_momentum,\n","                                                       training=self._training)\n","            inputs = self.activation(inputs, name=\"hidden%d_out\" % (layer + 1))\n","        return inputs\n","\n","    def _build_graph(self, n_inputs, n_outputs):\n","        \"\"\"Build the same model as earlier\"\"\"\n","        if self.random_state is not None:\n","            tf.set_random_seed(self.random_state)\n","            np.random.seed(self.random_state)\n","\n","        X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","        y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n","\n","        if self.batch_norm_momentum or self.dropout_rate:\n","            self._training = tf.placeholder_with_default(False, shape=(), name='training')\n","        else:\n","            self._training = None\n","\n","        dnn_outputs = self._dnn(X)\n","\n","        logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init, name=\"logits\")\n","        Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n","\n","        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n","                                                                  logits=logits)\n","        loss = tf.reduce_mean(xentropy, name=\"loss\")\n","\n","        optimizer = self.optimizer_class(learning_rate=self.learning_rate)\n","        training_op = optimizer.minimize(loss)\n","\n","        correct = tf.nn.in_top_k(logits, y, 1)\n","        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n","\n","        init = tf.global_variables_initializer()\n","        saver = tf.train.Saver()\n","\n","        # Make the important operations available easily through instance variables\n","        self._X, self._y = X, y\n","        self._Y_proba, self._loss = Y_proba, loss\n","        self._training_op, self._accuracy = training_op, accuracy\n","        self._init, self._saver = init, saver\n","\n","    def close_session(self):\n","        if self._session:\n","            self._session.close()\n","\n","    def _get_model_params(self):\n","        \"\"\"Get all variable values (used for early stopping, faster than saving to disk)\"\"\"\n","        with self._graph.as_default():\n","            gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n","        return {gvar.op.name: value for gvar, value in zip(gvars, self._session.run(gvars))}\n","\n","    def _restore_model_params(self, model_params):\n","        \"\"\"Set all variables to the given values (for early stopping, faster than loading from disk)\"\"\"\n","        gvar_names = list(model_params.keys())\n","        assign_ops = {gvar_name: self._graph.get_operation_by_name(gvar_name + \"/Assign\")\n","                      for gvar_name in gvar_names}\n","        init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n","        feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n","        self._session.run(assign_ops, feed_dict=feed_dict)\n","\n","    def fit(self, X, y, n_epochs=100, X_valid=None, y_valid=None):\n","        \"\"\"Fit the model to the training set. If X_valid and y_valid are provided, use early stopping.\"\"\"\n","        self.close_session()\n","\n","        # infer n_inputs and n_outputs from the training set.\n","        n_inputs = X.shape[1]\n","        self.classes_ = np.unique(y)\n","        n_outputs = len(self.classes_)\n","        \n","        # Translate the labels vector to a vector of sorted class indices, containing\n","        # integers from 0 to n_outputs - 1.\n","        # For example, if y is equal to [8, 8, 9, 5, 7, 6, 6, 6], then the sorted class\n","        # labels (self.classes_) will be equal to [5, 6, 7, 8, 9], and the labels vector\n","        # will be translated to [3, 3, 4, 0, 2, 1, 1, 1]\n","        self.class_to_index_ = {label: index\n","                                for index, label in enumerate(self.classes_)}\n","        y = np.array([self.class_to_index_[label]\n","                      for label in y], dtype=np.int32)\n","        \n","        self._graph = tf.Graph()\n","        with self._graph.as_default():\n","            self._build_graph(n_inputs, n_outputs)\n","            # extra ops for batch normalization\n","            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n","\n","        # needed in case of early stopping\n","        max_checks_without_progress = 20\n","        checks_without_progress = 0\n","        best_loss = np.infty\n","        best_params = None\n","        \n","        # Now train the model!\n","        self._session = tf.Session(graph=self._graph)\n","        with self._session.as_default() as sess:\n","            self._init.run()\n","            for epoch in range(n_epochs):\n","                rnd_idx = np.random.permutation(len(X))\n","                for rnd_indices in np.array_split(rnd_idx, len(X) // self.batch_size):\n","                    X_batch, y_batch = X[rnd_indices], y[rnd_indices]\n","                    feed_dict = {self._X: X_batch, self._y: y_batch}\n","                    if self._training is not None:\n","                        feed_dict[self._training] = True\n","                    sess.run(self._training_op, feed_dict=feed_dict)\n","                    if extra_update_ops:\n","                        sess.run(extra_update_ops, feed_dict=feed_dict)\n","                if X_valid is not None and y_valid is not None:\n","                    loss_val, acc_val = sess.run([self._loss, self._accuracy],\n","                                                 feed_dict={self._X: X_valid,\n","                                                            self._y: y_valid})\n","                    if loss_val < best_loss:\n","                        best_params = self._get_model_params()\n","                        best_loss = loss_val\n","                        checks_without_progress = 0\n","                    else:\n","                        checks_without_progress += 1\n","                    print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n","                        epoch, loss_val, best_loss, acc_val * 100))\n","                    if checks_without_progress > max_checks_without_progress:\n","                        print(\"Early stopping!\")\n","                        break\n","                else:\n","                    loss_train, acc_train = sess.run([self._loss, self._accuracy],\n","                                                     feed_dict={self._X: X_batch,\n","                                                                self._y: y_batch})\n","                    print(\"{}\\tLast training batch loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n","                        epoch, loss_train, acc_train * 100))\n","            # If we used early stopping then rollback to the best model found\n","            if best_params:\n","                self._restore_model_params(best_params)\n","            return self\n","\n","    def predict_proba(self, X):\n","        if not self._session:\n","            raise NotFittedError(\"This %s instance is not fitted yet\" % self.__class__.__name__)\n","        with self._session.as_default() as sess:\n","            return self._Y_proba.eval(feed_dict={self._X: X})\n","\n","    def predict(self, X):\n","        class_indices = np.argmax(self.predict_proba(X), axis=1)\n","        return np.array([[self.classes_[class_index]]\n","                         for class_index in class_indices], np.int32)\n","\n","    def save(self, path):\n","        self._saver.save(self._session, path)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Dq0-L6zqGDYc","colab_type":"text"},"cell_type":"markdown","source":["Let's see if we get the exact same accuracy as earlier using this class (without dropout or batch norm):"]},{"metadata":{"id":"2Amm2RlTGDYd","colab_type":"code","colab":{}},"cell_type":"code","source":["dnn_clf = DNNClassifier(random_state=42)\n","dnn_clf.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"v3qPw3iJGDYe","colab_type":"text"},"cell_type":"markdown","source":["The model is trained, let's see if it gets the same accuracy as earlier:"]},{"metadata":{"id":"7iVv1G9cGDYf","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","\n","y_pred = dnn_clf.predict(X_test1)\n","accuracy_score(y_test1, y_pred)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yJ4fap3DGDYh","colab_type":"text"},"cell_type":"markdown","source":["Yep! Working fine. Now we can use Scikit-Learn's `RandomizedSearchCV` class to search for better hyperparameters (this may take over an hour, depending on your system):"]},{"metadata":{"id":"k89L-2GfGDYh","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.model_selection import RandomizedSearchCV\n","\n","def leaky_relu(alpha=0.01):\n","    def parametrized_leaky_relu(z, name=None):\n","        return tf.maximum(alpha * z, z, name=name)\n","    return parametrized_leaky_relu\n","\n","param_distribs = {\n","    \"n_neurons\": [10, 30, 50, 70, 90, 100, 120, 140, 160],\n","    \"batch_size\": [10, 50, 100, 500],\n","    \"learning_rate\": [0.01, 0.02, 0.05, 0.1],\n","    \"activation\": [tf.nn.relu, tf.nn.elu, leaky_relu(alpha=0.01), leaky_relu(alpha=0.1)],\n","    # you could also try exploring different numbers of hidden layers, different optimizers, etc.\n","    #\"n_hidden_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n","    #\"optimizer_class\": [tf.train.AdamOptimizer, partial(tf.train.MomentumOptimizer, momentum=0.95)],\n","}\n","\n","rnd_search = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs, n_iter=50,\n","                                random_state=42, verbose=2)\n","rnd_search.fit(X_train1, y_train1, X_valid=X_valid1, y_valid=y_valid1, n_epochs=1000)\n","\n","# If you have Scikit-Learn 0.18 or earlier, you should upgrade, or use the fit_params argument:\n","# fit_params={\"X_valid\": X_valid1, \"y_valid\": y_valid1, \"n_epochs\": 1000}\n","# rnd_search = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs, n_iter=50,\n","#                                 fit_params=fit_params, random_state=42, verbose=2)\n","# rnd_search.fit(X_train1, y_train1)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ff-m_6MFGDYj","colab_type":"code","colab":{}},"cell_type":"code","source":["rnd_search.best_params_"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2caiBtVEGDYl","colab_type":"code","colab":{}},"cell_type":"code","source":["y_pred = rnd_search.predict(X_test1)\n","accuracy_score(y_test1, y_pred)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DSFKMAxyGDYo","colab_type":"text"},"cell_type":"markdown","source":["Wonderful! Tuning the hyperparameters got us up to 99.32% accuracy! It may not sound like a great improvement to go from 98.05% to 99.32% accuracy, but consider the error rate: it went from roughly 2% to 0.7%. That's a 65% reduction of the number of errors this model will produce!"]},{"metadata":{"id":"zLDfpXP9GDYo","colab_type":"text"},"cell_type":"markdown","source":["It's a good idea to save this model:"]},{"metadata":{"id":"VNQ-S8PrGDYp","colab_type":"code","colab":{}},"cell_type":"code","source":["rnd_search.best_estimator_.save(\"./my_best_mnist_model_0_to_4\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FKezmx2WGDYp","colab_type":"text"},"cell_type":"markdown","source":["### 8.4."]},{"metadata":{"id":"nCHFtwkWGDYp","colab_type":"text"},"cell_type":"markdown","source":["_Exercise: Now try adding Batch Normalization and compare the learning curves: is it converging faster than before? Does it produce a better model?_"]},{"metadata":{"id":"ZSfS7bNdGDYp","colab_type":"text"},"cell_type":"markdown","source":["Let's train the best model found, once again, to see how fast it converges (alternatively, you could tweak the code above to make it write summaries for TensorBoard, so you can visualize the learning curve):"]},{"metadata":{"id":"h2BcKDJOGDYq","colab_type":"code","colab":{}},"cell_type":"code","source":["dnn_clf = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n","                        n_neurons=140, random_state=42)\n","dnn_clf.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YoT2h6CUGDYr","colab_type":"text"},"cell_type":"markdown","source":["The best loss is reached at epoch 19, but it was already within 10% of that result at epoch 9."]},{"metadata":{"id":"2KssKeHSGDYr","colab_type":"text"},"cell_type":"markdown","source":["Let's check that we do indeed get 99.32% accuracy on the test set:"]},{"metadata":{"id":"V7Amxio6GDYr","colab_type":"code","colab":{}},"cell_type":"code","source":["y_pred = dnn_clf.predict(X_test1)\n","accuracy_score(y_test1, y_pred)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"k6-jfvG_GDYs","colab_type":"text"},"cell_type":"markdown","source":["Good, now let's use the exact same model, but this time with batch normalization:"]},{"metadata":{"id":"zbffi44lGDYs","colab_type":"code","colab":{}},"cell_type":"code","source":["dnn_clf_bn = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n","                           n_neurons=90, random_state=42,\n","                           batch_norm_momentum=0.95)\n","dnn_clf_bn.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Y_zTavqvGDYt","colab_type":"text"},"cell_type":"markdown","source":["The best params are reached during epoch 48, that's actually a slower convergence than earlier. Let's check the accuracy:"]},{"metadata":{"id":"sgbed6_SGDYt","colab_type":"code","colab":{}},"cell_type":"code","source":["y_pred = dnn_clf_bn.predict(X_test1)\n","accuracy_score(y_test1, y_pred)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3TizFEv1GDYu","colab_type":"text"},"cell_type":"markdown","source":["Well, batch normalization did not improve accuracy. Let's see if we can find a good set of hyperparameters that will work well with batch normalization:"]},{"metadata":{"id":"ITXPZ3w_GDYu","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.model_selection import RandomizedSearchCV\n","\n","param_distribs = {\n","    \"n_neurons\": [10, 30, 50, 70, 90, 100, 120, 140, 160],\n","    \"batch_size\": [10, 50, 100, 500],\n","    \"learning_rate\": [0.01, 0.02, 0.05, 0.1],\n","    \"activation\": [tf.nn.relu, tf.nn.elu, leaky_relu(alpha=0.01), leaky_relu(alpha=0.1)],\n","    # you could also try exploring different numbers of hidden layers, different optimizers, etc.\n","    #\"n_hidden_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n","    #\"optimizer_class\": [tf.train.AdamOptimizer, partial(tf.train.MomentumOptimizer, momentum=0.95)],\n","    \"batch_norm_momentum\": [0.9, 0.95, 0.98, 0.99, 0.999],\n","}\n","\n","rnd_search_bn = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs, n_iter=50,\n","                                   fit_params={\"X_valid\": X_valid1, \"y_valid\": y_valid1, \"n_epochs\": 1000},\n","                                   random_state=42, verbose=2)\n","rnd_search_bn.fit(X_train1, y_train1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dqMTrlDiGDYw","colab_type":"code","colab":{}},"cell_type":"code","source":["rnd_search_bn.best_params_"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AvPZsiDkGDYy","colab_type":"code","colab":{}},"cell_type":"code","source":["y_pred = rnd_search_bn.predict(X_test1)\n","accuracy_score(y_test1, y_pred)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"43PE8KYmGDY0","colab_type":"text"},"cell_type":"markdown","source":["Slightly better than earlier: 99.4% vs 99.3%. Let's see if dropout can do better."]},{"metadata":{"id":"3sPOnZxRGDY0","colab_type":"text"},"cell_type":"markdown","source":["### 8.5."]},{"metadata":{"id":"5CProHdtGDY0","colab_type":"text"},"cell_type":"markdown","source":["_Exercise: is the model overfitting the training set? Try adding dropout to every layer and try again. Does it help?_"]},{"metadata":{"id":"0UAMwEIDGDY1","colab_type":"text"},"cell_type":"markdown","source":["Let's go back to the best model we trained earlier and see how it performs on the training set:"]},{"metadata":{"id":"U8EKZaGOGDY2","colab_type":"code","colab":{}},"cell_type":"code","source":["y_pred = dnn_clf.predict(X_train1)\n","accuracy_score(y_train1, y_pred)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6YaSc8xNGDY4","colab_type":"text"},"cell_type":"markdown","source":["The model performs significantly better on the training set than on the test set (99.91% vs 99.32%), which means it is overfitting the training set. A bit of regularization may help. Let's try adding dropout with a 50% dropout rate:"]},{"metadata":{"id":"hD5j5hzBGDY4","colab_type":"code","colab":{}},"cell_type":"code","source":["dnn_clf_dropout = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n","                                n_neurons=90, random_state=42,\n","                                dropout_rate=0.5)\n","dnn_clf_dropout.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9cRZvJIYGDY6","colab_type":"text"},"cell_type":"markdown","source":["The best params are reached during epoch 23. Dropout somewhat slowed down convergence."]},{"metadata":{"id":"Is1DhKydGDY6","colab_type":"text"},"cell_type":"markdown","source":["Let's check the accuracy:"]},{"metadata":{"id":"TpLTBq0RGDY6","colab_type":"code","colab":{}},"cell_type":"code","source":["y_pred = dnn_clf_dropout.predict(X_test1)\n","accuracy_score(y_test1, y_pred)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2r3hxwY_GDY7","colab_type":"text"},"cell_type":"markdown","source":["We are out of luck, dropout does not seem to help either. Let's try tuning the hyperparameters, perhaps we can squeeze a bit more performance out of this model:"]},{"metadata":{"id":"5-xhavbNGDY7","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.model_selection import RandomizedSearchCV\n","\n","param_distribs = {\n","    \"n_neurons\": [10, 30, 50, 70, 90, 100, 120, 140, 160],\n","    \"batch_size\": [10, 50, 100, 500],\n","    \"learning_rate\": [0.01, 0.02, 0.05, 0.1],\n","    \"activation\": [tf.nn.relu, tf.nn.elu, leaky_relu(alpha=0.01), leaky_relu(alpha=0.1)],\n","    # you could also try exploring different numbers of hidden layers, different optimizers, etc.\n","    #\"n_hidden_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n","    #\"optimizer_class\": [tf.train.AdamOptimizer, partial(tf.train.MomentumOptimizer, momentum=0.95)],\n","    \"dropout_rate\": [0.2, 0.3, 0.4, 0.5, 0.6],\n","}\n","\n","rnd_search_dropout = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs, n_iter=50,\n","                                        fit_params={\"X_valid\": X_valid1, \"y_valid\": y_valid1, \"n_epochs\": 1000},\n","                                        random_state=42, verbose=2)\n","rnd_search_dropout.fit(X_train1, y_train1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6I8tNOoAGDY_","colab_type":"code","colab":{}},"cell_type":"code","source":["rnd_search_dropout.best_params_"],"execution_count":0,"outputs":[]},{"metadata":{"id":"genB1uEVGDZN","colab_type":"code","colab":{}},"cell_type":"code","source":["y_pred = rnd_search_dropout.predict(X_test1)\n","accuracy_score(y_test1, y_pred)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mTM_v46nGDZQ","colab_type":"text"},"cell_type":"markdown","source":["Oh well, dropout did not improve the model. Better luck next time! :)"]},{"metadata":{"id":"TGBP7AM-GDZQ","colab_type":"text"},"cell_type":"markdown","source":["But that's okay, we have ourselves a nice DNN that achieves 99.40% accuracy on the test set using Batch Normalization, or 99.32% without BN. Let's see if some of this expertise on digits 0 to 4 can be transferred to the task of classifying digits 5 to 9. For the sake of simplicity we will reuse the DNN without BN, since it is almost as good."]},{"metadata":{"collapsed":true,"id":"2VWtSSVdGDZQ","colab_type":"text"},"cell_type":"markdown","source":["## 9. Transfer learning"]},{"metadata":{"id":"UE2u0bryGDZR","colab_type":"text"},"cell_type":"markdown","source":["### 9.1."]},{"metadata":{"id":"6jjWlrsVGDZR","colab_type":"text"},"cell_type":"markdown","source":["_Exercise: create a new DNN that reuses all the pretrained hidden layers of the previous model, freezes them, and replaces the softmax output layer with a new one._"]},{"metadata":{"id":"5QIay0C4GDZR","colab_type":"text"},"cell_type":"markdown","source":["Let's load the best model's graph and get a handle on all the important operations we will need. Note that instead of creating a new softmax output layer, we will just reuse the existing one (since it has the same number of outputs as the existing one). We will reinitialize its parameters before training. "]},{"metadata":{"id":"BHOuDacCGDZS","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","restore_saver = tf.train.import_meta_graph(\"./my_best_mnist_model_0_to_4.meta\")\n","\n","X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n","y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n","loss = tf.get_default_graph().get_tensor_by_name(\"loss:0\")\n","Y_proba = tf.get_default_graph().get_tensor_by_name(\"Y_proba:0\")\n","logits = Y_proba.op.inputs[0]\n","accuracy = tf.get_default_graph().get_tensor_by_name(\"accuracy:0\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CbAbDBJhGDZS","colab_type":"text"},"cell_type":"markdown","source":["To freeze the lower layers, we will exclude their variables from the optimizer's list of trainable variables, keeping only the output layer's trainable variables:"]},{"metadata":{"id":"5_CpEnADGDZS","colab_type":"code","colab":{}},"cell_type":"code","source":["learning_rate = 0.01\n","\n","output_layer_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"logits\")\n","optimizer = tf.train.AdamOptimizer(learning_rate, name=\"Adam2\")\n","training_op = optimizer.minimize(loss, var_list=output_layer_vars)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UYz2CBhWGDZT","colab_type":"code","colab":{}},"cell_type":"code","source":["correct = tf.nn.in_top_k(logits, y, 1)\n","accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n","\n","init = tf.global_variables_initializer()\n","five_frozen_saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"38wQVU2NGDZT","colab_type":"text"},"cell_type":"markdown","source":["### 9.2."]},{"metadata":{"id":"O8QRUDIjGDZa","colab_type":"text"},"cell_type":"markdown","source":["_Exercise: train this new DNN on digits 5 to 9, using only 100 images per digit, and time how long it takes. Despite this small number of examples, can you achieve high precision?_"]},{"metadata":{"id":"Uf-FQblZGDZb","colab_type":"text"},"cell_type":"markdown","source":["Let's create the training, validation and test sets. We need to subtract 5 from the labels because TensorFlow expects integers from 0 to `n_classes-1`."]},{"metadata":{"id":"zivnsrM6GDZb","colab_type":"code","colab":{}},"cell_type":"code","source":["X_train2_full = X_train[y_train >= 5]\n","y_train2_full = y_train[y_train >= 5] - 5\n","X_valid2_full = X_valid[y_valid >= 5]\n","y_valid2_full = y_valid[y_valid >= 5] - 5\n","X_test2 = X_test[y_test >= 5]\n","y_test2 = y_test[y_test >= 5] - 5"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1sCkBAs2GDZd","colab_type":"text"},"cell_type":"markdown","source":["Also, for the purpose of this exercise, we want to keep only 100 instances per class in the training set (and let's keep only 30 instances per class in the validation set). Let's create a small function to do that:"]},{"metadata":{"id":"TFP4jLenGDZe","colab_type":"code","colab":{}},"cell_type":"code","source":["def sample_n_instances_per_class(X, y, n=100):\n","    Xs, ys = [], []\n","    for label in np.unique(y):\n","        idx = (y == label)\n","        Xc = X[idx][:n]\n","        yc = y[idx][:n]\n","        Xs.append(Xc)\n","        ys.append(yc)\n","    return np.concatenate(Xs), np.concatenate(ys)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lddRimt6GDZh","colab_type":"code","colab":{}},"cell_type":"code","source":["X_train2, y_train2 = sample_n_instances_per_class(X_train2_full, y_train2_full, n=100)\n","X_valid2, y_valid2 = sample_n_instances_per_class(X_valid2_full, y_valid2_full, n=30)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6qNEVN4yGDZi","colab_type":"text"},"cell_type":"markdown","source":["Now let's train the model. This is the same training code as earlier, using early stopping, except for the initialization: we first initialize all the variables, then we restore the best model trained earlier (on digits 0 to 4), and finally we reinitialize the output layer variables."]},{"metadata":{"id":"D-uSszvBGDZi","colab_type":"code","colab":{}},"cell_type":"code","source":["import time\n","\n","n_epochs = 1000\n","batch_size = 20\n","\n","max_checks_without_progress = 20\n","checks_without_progress = 0\n","best_loss = np.infty\n","\n","with tf.Session() as sess:\n","    init.run()\n","    restore_saver.restore(sess, \"./my_best_mnist_model_0_to_4\")\n","    t0 = time.time()\n","        \n","    for epoch in range(n_epochs):\n","        rnd_idx = np.random.permutation(len(X_train2))\n","        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n","            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid2, y: y_valid2})\n","        if loss_val < best_loss:\n","            save_path = five_frozen_saver.save(sess, \"./my_mnist_model_5_to_9_five_frozen\")\n","            best_loss = loss_val\n","            checks_without_progress = 0\n","        else:\n","            checks_without_progress += 1\n","            if checks_without_progress > max_checks_without_progress:\n","                print(\"Early stopping!\")\n","                break\n","        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n","            epoch, loss_val, best_loss, acc_val * 100))\n","\n","    t1 = time.time()\n","    print(\"Total training time: {:.1f}s\".format(t1 - t0))\n","\n","with tf.Session() as sess:\n","    five_frozen_saver.restore(sess, \"./my_mnist_model_5_to_9_five_frozen\")\n","    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n","    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"u63gdYK8GDZn","colab_type":"text"},"cell_type":"markdown","source":["Well that's not a great accuracy, is it? Of course with such a tiny training set, and with only one layer to tweak, we should not expect miracles."]},{"metadata":{"id":"RrZ-DB--GDZp","colab_type":"text"},"cell_type":"markdown","source":["### 9.3."]},{"metadata":{"id":"9Rx9kWqwGDZq","colab_type":"text"},"cell_type":"markdown","source":["_Exercise: try caching the frozen layers, and train the model again: how much faster is it now?_"]},{"metadata":{"id":"s8opa_bkGDZq","colab_type":"text"},"cell_type":"markdown","source":["Let's start by getting a handle on the output of the last frozen layer:"]},{"metadata":{"id":"_gLn6K11GDZr","colab_type":"code","colab":{}},"cell_type":"code","source":["hidden5_out = tf.get_default_graph().get_tensor_by_name(\"hidden5_out:0\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"f_Sdhuv7GDZr","colab_type":"text"},"cell_type":"markdown","source":["Now let's train the model using roughly the same code as earlier. The difference is that we compute the output of the top frozen layer at the beginning (both for the training set and the validation set), and we cache it. This makes training roughly 1.5 to 3 times faster in this example (this may vary greatly, depending on your system): "]},{"metadata":{"id":"sp_HVz4_GDZu","colab_type":"code","colab":{}},"cell_type":"code","source":["import time\n","\n","n_epochs = 1000\n","batch_size = 20\n","\n","max_checks_without_progress = 20\n","checks_without_progress = 0\n","best_loss = np.infty\n","\n","with tf.Session() as sess:\n","    init.run()\n","    restore_saver.restore(sess, \"./my_best_mnist_model_0_to_4\")\n","    t0 = time.time()\n","    \n","    hidden5_train = hidden5_out.eval(feed_dict={X: X_train2, y: y_train2})\n","    hidden5_valid = hidden5_out.eval(feed_dict={X: X_valid2, y: y_valid2})\n","        \n","    for epoch in range(n_epochs):\n","        rnd_idx = np.random.permutation(len(X_train2))\n","        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n","            h5_batch, y_batch = hidden5_train[rnd_indices], y_train2[rnd_indices]\n","            sess.run(training_op, feed_dict={hidden5_out: h5_batch, y: y_batch})\n","        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={hidden5_out: hidden5_valid, y: y_valid2})\n","        if loss_val < best_loss:\n","            save_path = five_frozen_saver.save(sess, \"./my_mnist_model_5_to_9_five_frozen\")\n","            best_loss = loss_val\n","            checks_without_progress = 0\n","        else:\n","            checks_without_progress += 1\n","            if checks_without_progress > max_checks_without_progress:\n","                print(\"Early stopping!\")\n","                break\n","        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n","            epoch, loss_val, best_loss, acc_val * 100))\n","\n","    t1 = time.time()\n","    print(\"Total training time: {:.1f}s\".format(t1 - t0))\n","\n","with tf.Session() as sess:\n","    five_frozen_saver.restore(sess, \"./my_mnist_model_5_to_9_five_frozen\")\n","    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n","    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Dybmldp-GDZx","colab_type":"text"},"cell_type":"markdown","source":["### 9.4."]},{"metadata":{"id":"swYrmiqYGDZy","colab_type":"text"},"cell_type":"markdown","source":["_Exercise: try again reusing just four hidden layers instead of five. Can you achieve a higher precision?_"]},{"metadata":{"id":"LdsQvX-9GDZy","colab_type":"text"},"cell_type":"markdown","source":["Let's load the best model again, but this time we will create a new softmax output layer on top of the 4th hidden layer:"]},{"metadata":{"id":"bemLJvQtGDZz","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_outputs = 5\n","\n","restore_saver = tf.train.import_meta_graph(\"./my_best_mnist_model_0_to_4.meta\")\n","\n","X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n","y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n","\n","hidden4_out = tf.get_default_graph().get_tensor_by_name(\"hidden4_out:0\")\n","logits = tf.layers.dense(hidden4_out, n_outputs, kernel_initializer=he_init, name=\"new_logits\")\n","Y_proba = tf.nn.softmax(logits)\n","xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","loss = tf.reduce_mean(xentropy)\n","correct = tf.nn.in_top_k(logits, y, 1)\n","accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_c7VxHJBGDZ1","colab_type":"text"},"cell_type":"markdown","source":["And now let's create the training operation. We want to freeze all the layers except for the new output layer:"]},{"metadata":{"id":"64bwlsW0GDZ1","colab_type":"code","colab":{}},"cell_type":"code","source":["learning_rate = 0.01\n","\n","output_layer_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"new_logits\")\n","optimizer = tf.train.AdamOptimizer(learning_rate, name=\"Adam2\")\n","training_op = optimizer.minimize(loss, var_list=output_layer_vars)\n","\n","init = tf.global_variables_initializer()\n","four_frozen_saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nsrCEk8GGDZ2","colab_type":"text"},"cell_type":"markdown","source":["And once again we train the model with the same code as earlier. Note: we could of course write a function once and use it multiple times, rather than copying almost the same training code over and over again, but as we keep tweaking the code slightly, the function would need multiple arguments and `if` statements, and it would have to be at the beginning of the notebook, where it would not make much sense to readers. In short it would be very confusing, so we're better off with copy & paste."]},{"metadata":{"id":"9ARrcyC5GDZ3","colab_type":"code","colab":{}},"cell_type":"code","source":["n_epochs = 1000\n","batch_size = 20\n","\n","max_checks_without_progress = 20\n","checks_without_progress = 0\n","best_loss = np.infty\n","\n","with tf.Session() as sess:\n","    init.run()\n","    restore_saver.restore(sess, \"./my_best_mnist_model_0_to_4\")\n","        \n","    for epoch in range(n_epochs):\n","        rnd_idx = np.random.permutation(len(X_train2))\n","        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n","            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid2, y: y_valid2})\n","        if loss_val < best_loss:\n","            save_path = four_frozen_saver.save(sess, \"./my_mnist_model_5_to_9_four_frozen\")\n","            best_loss = loss_val\n","            checks_without_progress = 0\n","        else:\n","            checks_without_progress += 1\n","            if checks_without_progress > max_checks_without_progress:\n","                print(\"Early stopping!\")\n","                break\n","        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n","            epoch, loss_val, best_loss, acc_val * 100))\n","\n","with tf.Session() as sess:\n","    four_frozen_saver.restore(sess, \"./my_mnist_model_5_to_9_four_frozen\")\n","    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n","    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iuf7t7UbGDZ6","colab_type":"text"},"cell_type":"markdown","source":["Still not fantastic, but much better."]},{"metadata":{"id":"Za9mrOeUGDZ6","colab_type":"text"},"cell_type":"markdown","source":["### 9.5."]},{"metadata":{"id":"3JI7O_-oGDZ7","colab_type":"text"},"cell_type":"markdown","source":["_Exercise: now unfreeze the top two hidden layers and continue training: can you get the model to perform even better?_"]},{"metadata":{"id":"Pk5tkr51GDZ7","colab_type":"code","colab":{}},"cell_type":"code","source":["learning_rate = 0.01\n","\n","unfrozen_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"hidden[34]|new_logits\")\n","optimizer = tf.train.AdamOptimizer(learning_rate, name=\"Adam3\")\n","training_op = optimizer.minimize(loss, var_list=unfrozen_vars)\n","\n","init = tf.global_variables_initializer()\n","two_frozen_saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5MSchoe-GDZ8","colab_type":"code","colab":{}},"cell_type":"code","source":["n_epochs = 1000\n","batch_size = 20\n","\n","max_checks_without_progress = 20\n","checks_without_progress = 0\n","best_loss = np.infty\n","\n","with tf.Session() as sess:\n","    init.run()\n","    four_frozen_saver.restore(sess, \"./my_mnist_model_5_to_9_four_frozen\")\n","        \n","    for epoch in range(n_epochs):\n","        rnd_idx = np.random.permutation(len(X_train2))\n","        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n","            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid2, y: y_valid2})\n","        if loss_val < best_loss:\n","            save_path = two_frozen_saver.save(sess, \"./my_mnist_model_5_to_9_two_frozen\")\n","            best_loss = loss_val\n","            checks_without_progress = 0\n","        else:\n","            checks_without_progress += 1\n","            if checks_without_progress > max_checks_without_progress:\n","                print(\"Early stopping!\")\n","                break\n","        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n","            epoch, loss_val, best_loss, acc_val * 100))\n","\n","with tf.Session() as sess:\n","    two_frozen_saver.restore(sess, \"./my_mnist_model_5_to_9_two_frozen\")\n","    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n","    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bjFzIWQUGDZ-","colab_type":"text"},"cell_type":"markdown","source":["Let's check what accuracy we can get by unfreezing all layers:"]},{"metadata":{"id":"UjybdmI0GDZ_","colab_type":"code","colab":{}},"cell_type":"code","source":["learning_rate = 0.01\n","\n","optimizer = tf.train.AdamOptimizer(learning_rate, name=\"Adam4\")\n","training_op = optimizer.minimize(loss)\n","\n","init = tf.global_variables_initializer()\n","no_frozen_saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KUrZGeShGDZ_","colab_type":"code","colab":{}},"cell_type":"code","source":["n_epochs = 1000\n","batch_size = 20\n","\n","max_checks_without_progress = 20\n","checks_without_progress = 0\n","best_loss = np.infty\n","\n","with tf.Session() as sess:\n","    init.run()\n","    two_frozen_saver.restore(sess, \"./my_mnist_model_5_to_9_two_frozen\")\n","        \n","    for epoch in range(n_epochs):\n","        rnd_idx = np.random.permutation(len(X_train2))\n","        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n","            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid2, y: y_valid2})\n","        if loss_val < best_loss:\n","            save_path = no_frozen_saver.save(sess, \"./my_mnist_model_5_to_9_no_frozen\")\n","            best_loss = loss_val\n","            checks_without_progress = 0\n","        else:\n","            checks_without_progress += 1\n","            if checks_without_progress > max_checks_without_progress:\n","                print(\"Early stopping!\")\n","                break\n","        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n","            epoch, loss_val, best_loss, acc_val * 100))\n","\n","with tf.Session() as sess:\n","    no_frozen_saver.restore(sess, \"./my_mnist_model_5_to_9_no_frozen\")\n","    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n","    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"i3Xy6jnDGDaB","colab_type":"text"},"cell_type":"markdown","source":["Let's compare that to a DNN trained from scratch:"]},{"metadata":{"id":"XyeWitFyGDaB","colab_type":"code","colab":{}},"cell_type":"code","source":["dnn_clf_5_to_9 = DNNClassifier(n_hidden_layers=4, random_state=42)\n","dnn_clf_5_to_9.fit(X_train2, y_train2, n_epochs=1000, X_valid=X_valid2, y_valid=y_valid2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DlAo-BLOGDaC","colab_type":"code","colab":{}},"cell_type":"code","source":["y_pred = dnn_clf_5_to_9.predict(X_test2)\n","accuracy_score(y_test2, y_pred)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HxJk_j_9GDaD","colab_type":"text"},"cell_type":"markdown","source":["Meh. How disappointing! ;) Transfer learning did not help much (if at all) in this task. At least we tried... Fortunately, the next exercise will get better results."]},{"metadata":{"id":"I8seniQMGDaD","colab_type":"text"},"cell_type":"markdown","source":["## 10. Pretraining on an auxiliary task"]},{"metadata":{"id":"VI-gMSkGGDaE","colab_type":"text"},"cell_type":"markdown","source":["In this exercise you will build a DNN that compares two MNIST digit images and predicts whether they represent the same digit or not. Then you will reuse the lower layers of this network to train an MNIST classifier using very little training data."]},{"metadata":{"id":"ebOVr4-eGDaE","colab_type":"text"},"cell_type":"markdown","source":["### 10.1.\n","Exercise: _Start by building two DNNs (let's call them DNN A and B), both similar to the one you built earlier but without the output layer: each DNN should have five hidden layers of 100 neurons each, He initialization, and ELU activation. Next, add one more hidden layer with 10 units on top of both DNNs. You should use TensorFlow's `concat()` function with `axis=1` to concatenate the outputs of both DNNs along the horizontal axis, then feed the result to the hidden layer. Finally, add an output layer with a single neuron using the logistic activation function._"]},{"metadata":{"id":"jjJj-T5MGDaE","colab_type":"text"},"cell_type":"markdown","source":["**Warning**! There was an error in the book for this exercise: there was no instruction to add a top hidden layer. Without it, the neural network generally fails to start learning. If you have the latest version of the book, this error has been fixed."]},{"metadata":{"id":"KzMdEE68GDaE","colab_type":"text"},"cell_type":"markdown","source":["You could have two input placeholders, `X1` and `X2`, one for the images that should be fed to the first DNN, and the other for the images that should be fed to the second DNN. It would work fine. However, another option is to have a single input placeholder to hold both sets of images (each row will hold a pair of images), and use `tf.unstack()` to split this tensor into two separate tensors, like this:"]},{"metadata":{"id":"-dUN2pptGDaE","colab_type":"code","colab":{}},"cell_type":"code","source":["n_inputs = 28 * 28 # MNIST\n","\n","reset_graph()\n","\n","X = tf.placeholder(tf.float32, shape=(None, 2, n_inputs), name=\"X\")\n","X1, X2 = tf.unstack(X, axis=1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cL8zL-KtGDaF","colab_type":"text"},"cell_type":"markdown","source":["We also need the labels placeholder. Each label will be 0 if the images represent different digits, or 1 if they represent the same digit:"]},{"metadata":{"id":"ZReZoDIJGDaF","colab_type":"code","colab":{}},"cell_type":"code","source":["y = tf.placeholder(tf.int32, shape=[None, 1])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"i8sOe3AfGDaG","colab_type":"text"},"cell_type":"markdown","source":["Now let's feed these inputs through two separate DNNs:"]},{"metadata":{"id":"PpJ4zxK5GDaG","colab_type":"code","colab":{}},"cell_type":"code","source":["dnn1 = dnn(X1, name=\"DNN_A\")\n","dnn2 = dnn(X2, name=\"DNN_B\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PmRTdnr1GDaH","colab_type":"text"},"cell_type":"markdown","source":["And let's concatenate their outputs:"]},{"metadata":{"id":"NhMRK3SmGDaH","colab_type":"code","colab":{}},"cell_type":"code","source":["dnn_outputs = tf.concat([dnn1, dnn2], axis=1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"y7BHluwuGDaI","colab_type":"text"},"cell_type":"markdown","source":["Each DNN outputs 100 activations (per instance), so the shape is `[None, 100]`:"]},{"metadata":{"id":"Wb4pBV01GDaO","colab_type":"code","colab":{}},"cell_type":"code","source":["dnn1.shape"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Dsr0A3NSGDaT","colab_type":"code","colab":{}},"cell_type":"code","source":["dnn2.shape"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gQrafHvqGDaV","colab_type":"text"},"cell_type":"markdown","source":["And of course the concatenated outputs have a shape of `[None, 200]`:"]},{"metadata":{"id":"6OBNinfAGDaV","colab_type":"code","colab":{}},"cell_type":"code","source":["dnn_outputs.shape"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7ImXsaFZGDaW","colab_type":"text"},"cell_type":"markdown","source":["Now lets add an extra hidden layer with just 10 neurons, and the output layer, with a single neuron:"]},{"metadata":{"id":"icKqBvZjGDaW","colab_type":"code","colab":{}},"cell_type":"code","source":["hidden = tf.layers.dense(dnn_outputs, units=10, activation=tf.nn.elu, kernel_initializer=he_init)\n","logits = tf.layers.dense(hidden, units=1, kernel_initializer=he_init)\n","y_proba = tf.nn.sigmoid(logits)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"68aQoP5QGDaX","colab_type":"text"},"cell_type":"markdown","source":["The whole network predicts `1` if `y_proba >= 0.5` (i.e. the network predicts that the images represent the same digit), or `0` otherwise. We compute instead `logits >= 0`, which is equivalent but faster to compute: "]},{"metadata":{"id":"FL6HlBWTGDaY","colab_type":"code","colab":{}},"cell_type":"code","source":["y_pred = tf.cast(tf.greater_equal(logits, 0), tf.int32)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HfedFzlZGDaa","colab_type":"text"},"cell_type":"markdown","source":["Now let's add the cost function:"]},{"metadata":{"id":"4KA9B5VpGDaa","colab_type":"code","colab":{}},"cell_type":"code","source":["y_as_float = tf.cast(y, tf.float32)\n","xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_as_float, logits=logits)\n","loss = tf.reduce_mean(xentropy)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"M9hkEZPPGDab","colab_type":"text"},"cell_type":"markdown","source":["And we can now create the training operation using an optimizer:"]},{"metadata":{"id":"BJjj1rxNGDab","colab_type":"code","colab":{}},"cell_type":"code","source":["learning_rate = 0.01\n","momentum = 0.95\n","\n","optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n","training_op = optimizer.minimize(loss)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5f1K6skmGDac","colab_type":"text"},"cell_type":"markdown","source":["We will want to measure our classifier's accuracy."]},{"metadata":{"id":"nqKogAfOGDac","colab_type":"code","colab":{}},"cell_type":"code","source":["y_pred_correct = tf.equal(y_pred, y)\n","accuracy = tf.reduce_mean(tf.cast(y_pred_correct, tf.float32))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"m8OdoSpxGDad","colab_type":"text"},"cell_type":"markdown","source":["And the usual `init` and `saver`:"]},{"metadata":{"id":"FOEblhsiGDad","colab_type":"code","colab":{}},"cell_type":"code","source":["init = tf.global_variables_initializer()\n","saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FP0FnUmiGDae","colab_type":"text"},"cell_type":"markdown","source":["### 10.2.\n","_Exercise: split the MNIST training set in two sets: split #1 should containing 55,000 images, and split #2 should contain contain 5,000 images. Create a function that generates a training batch where each instance is a pair of MNIST images picked from split #1. Half of the training instances should be pairs of images that belong to the same class, while the other half should be images from different classes. For each pair, the training label should be 0 if the images are from the same class, or 1 if they are from different classes._"]},{"metadata":{"id":"SGm11qL0GDae","colab_type":"text"},"cell_type":"markdown","source":["The MNIST dataset returned by TensorFlow's `input_data()` function is already split into 3 parts: a training set (55,000 instances), a validation set (5,000 instances) and a test set (10,000 instances). Let's use the first set to generate the training set composed image pairs, and we will use the second set for the second phase of the exercise (to train a regular MNIST classifier). We will use the third set as the test set for both phases."]},{"metadata":{"id":"zhGbd-frGDaf","colab_type":"code","colab":{}},"cell_type":"code","source":["X_train1 = X_train\n","y_train1 = y_train\n","\n","X_train2 = X_valid\n","y_train2 = y_valid\n","\n","X_test = X_test\n","y_test = y_test"],"execution_count":0,"outputs":[]},{"metadata":{"id":"p5PfbQbSGDaf","colab_type":"text"},"cell_type":"markdown","source":["Let's write a function that generates pairs of images: 50% representing the same digit, and 50% representing different digits. There are many ways to implement this. In this implementation, we first decide how many \"same\" pairs (i.e. pairs of images representing the same digit) we will generate, and how many \"different\" pairs (i.e. pairs of images representing different digits). We could just use `batch_size // 2` but we want to handle the case where it is odd (granted, that might be overkill!). Then we generate random pairs and we pick the right number of \"same\" pairs, then we generate the right number of \"different\" pairs. Finally we shuffle the batch and return it:"]},{"metadata":{"id":"SfbLEiUyGDaf","colab_type":"code","colab":{}},"cell_type":"code","source":["def generate_batch(images, labels, batch_size):\n","    size1 = batch_size // 2\n","    size2 = batch_size - size1\n","    if size1 != size2 and np.random.rand() > 0.5:\n","        size1, size2 = size2, size1\n","    X = []\n","    y = []\n","    while len(X) < size1:\n","        rnd_idx1, rnd_idx2 = np.random.randint(0, len(images), 2)\n","        if rnd_idx1 != rnd_idx2 and labels[rnd_idx1] == labels[rnd_idx2]:\n","            X.append(np.array([images[rnd_idx1], images[rnd_idx2]]))\n","            y.append([1])\n","    while len(X) < batch_size:\n","        rnd_idx1, rnd_idx2 = np.random.randint(0, len(images), 2)\n","        if labels[rnd_idx1] != labels[rnd_idx2]:\n","            X.append(np.array([images[rnd_idx1], images[rnd_idx2]]))\n","            y.append([0])\n","    rnd_indices = np.random.permutation(batch_size)\n","    return np.array(X)[rnd_indices], np.array(y)[rnd_indices]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eYgb0adJGDah","colab_type":"text"},"cell_type":"markdown","source":["Let's test it to generate a small batch of 5 image pairs:"]},{"metadata":{"id":"QPRiSL-5GDai","colab_type":"code","colab":{}},"cell_type":"code","source":["batch_size = 5\n","X_batch, y_batch = generate_batch(X_train1, y_train1, batch_size)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fBSAXCmCGDak","colab_type":"text"},"cell_type":"markdown","source":["Each row in `X_batch` contains a pair of images:"]},{"metadata":{"id":"VOSgTlzCGDal","colab_type":"code","colab":{}},"cell_type":"code","source":["X_batch.shape, X_batch.dtype"],"execution_count":0,"outputs":[]},{"metadata":{"id":"A5C6PrO0GDam","colab_type":"text"},"cell_type":"markdown","source":["Let's look at these pairs:"]},{"metadata":{"id":"f74JVjZzGDam","colab_type":"code","colab":{}},"cell_type":"code","source":["plt.figure(figsize=(3, 3 * batch_size))\n","plt.subplot(121)\n","plt.imshow(X_batch[:,0].reshape(28 * batch_size, 28), cmap=\"binary\", interpolation=\"nearest\")\n","plt.axis('off')\n","plt.subplot(122)\n","plt.imshow(X_batch[:,1].reshape(28 * batch_size, 28), cmap=\"binary\", interpolation=\"nearest\")\n","plt.axis('off')\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DFLewIYlGDan","colab_type":"text"},"cell_type":"markdown","source":["And let's look at the labels (0 means \"different\", 1 means \"same\"):"]},{"metadata":{"id":"xu0J1hoAGDao","colab_type":"code","colab":{}},"cell_type":"code","source":["y_batch"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RZPfYi8cGDap","colab_type":"text"},"cell_type":"markdown","source":["Perfect!"]},{"metadata":{"id":"RwxcuDbbGDaq","colab_type":"text"},"cell_type":"markdown","source":["### 10.3.\n","_Exercise: train the DNN on this training set. For each image pair, you can simultaneously feed the first image to DNN A and the second image to DNN B. The whole network will gradually learn to tell whether two images belong to the same class or not._"]},{"metadata":{"id":"pXwYGMfOGDar","colab_type":"text"},"cell_type":"markdown","source":["Let's generate a test set composed of many pairs of images pulled from the MNIST test set:"]},{"metadata":{"id":"g8M5vLhLGDar","colab_type":"code","colab":{}},"cell_type":"code","source":["X_test1, y_test1 = generate_batch(X_test, y_test, batch_size=len(X_test))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hOx70h-UGDas","colab_type":"text"},"cell_type":"markdown","source":["And now, let's train the model. There's really nothing special about this step, except for the fact that we need a fairly large `batch_size`, otherwise the model fails to learn anything and ends up with an accuracy of 50%:"]},{"metadata":{"id":"Ba2y1rUeGDas","colab_type":"code","colab":{}},"cell_type":"code","source":["n_epochs = 100\n","batch_size = 500\n","\n","with tf.Session() as sess:\n","    init.run()\n","    for epoch in range(n_epochs):\n","        for iteration in range(len(X_train1) // batch_size):\n","            X_batch, y_batch = generate_batch(X_train1, y_train1, batch_size)\n","            loss_val, _ = sess.run([loss, training_op], feed_dict={X: X_batch, y: y_batch})\n","        print(epoch, \"Train loss:\", loss_val)\n","        if epoch % 5 == 0:\n","            acc_test = accuracy.eval(feed_dict={X: X_test1, y: y_test1})\n","            print(epoch, \"Test accuracy:\", acc_test)\n","\n","    save_path = saver.save(sess, \"./my_digit_comparison_model.ckpt\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GxjCdBURGDat","colab_type":"text"},"cell_type":"markdown","source":["All right, we reach 97.6% accuracy on this digit comparison task. That's not too bad, this model knows a thing or two about comparing handwritten digits!\n","\n","Let's see if some of that knowledge can be useful for the regular MNIST classification task."]},{"metadata":{"id":"0FR4gMR7GDat","colab_type":"text"},"cell_type":"markdown","source":["### 10.4.\n","_Exercise: now create a new DNN by reusing and freezing the hidden layers of DNN A and adding a softmax output layer on top with 10 neurons. Train this network on split #2 and see if you can achieve high performance despite having only 500 images per class._"]},{"metadata":{"id":"iFY0P3MQGDau","colab_type":"text"},"cell_type":"markdown","source":["Let's create the model, it is pretty straightforward. There are many ways to freeze the lower layers, as explained in the book. In this example, we chose to use the `tf.stop_gradient()` function. Note that we need one `Saver` to restore the pretrained DNN A, and another `Saver` to save the final model: "]},{"metadata":{"id":"ozPAJ6ftGDau","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 28 * 28  # MNIST\n","n_outputs = 10\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n","\n","dnn_outputs = dnn(X, name=\"DNN_A\")\n","frozen_outputs = tf.stop_gradient(dnn_outputs)\n","\n","logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init)\n","Y_proba = tf.nn.softmax(logits)\n","\n","xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","loss = tf.reduce_mean(xentropy, name=\"loss\")\n","\n","optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n","training_op = optimizer.minimize(loss)\n","\n","correct = tf.nn.in_top_k(logits, y, 1)\n","accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n","\n","init = tf.global_variables_initializer()\n","\n","dnn_A_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"DNN_A\")\n","restore_saver = tf.train.Saver(var_list={var.op.name: var for var in dnn_A_vars})\n","saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"c2DPDq9NGDav","colab_type":"text"},"cell_type":"markdown","source":["Now on to training! We first initialize all variables (including the variables in the new output layer), then we restore the pretrained DNN A. Next, we just train the model on the small MNIST dataset (containing just 5,000 images):"]},{"metadata":{"id":"U0cANEguGDav","colab_type":"code","colab":{}},"cell_type":"code","source":["n_epochs = 100\n","batch_size = 50\n","\n","with tf.Session() as sess:\n","    init.run()\n","    restore_saver.restore(sess, \"./my_digit_comparison_model.ckpt\")\n","\n","    for epoch in range(n_epochs):\n","        rnd_idx = np.random.permutation(len(X_train2))\n","        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n","            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","        if epoch % 10 == 0:\n","            acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n","            print(epoch, \"Test accuracy:\", acc_test)\n","\n","    save_path = saver.save(sess, \"./my_mnist_model_final.ckpt\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XHXTcpJDGDaw","colab_type":"text"},"cell_type":"markdown","source":["Well, 96.7% accuracy, that's not the best MNIST model we have trained so far, but recall that we are only using a small training set (just 500 images per digit). Let's compare this result with the same DNN trained from scratch, without using transfer learning:"]},{"metadata":{"id":"zgi42YwTGDaw","colab_type":"code","colab":{}},"cell_type":"code","source":["reset_graph()\n","\n","n_inputs = 28 * 28  # MNIST\n","n_outputs = 10\n","\n","X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n","y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n","\n","dnn_outputs = dnn(X, name=\"DNN_A\")\n","\n","logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init)\n","Y_proba = tf.nn.softmax(logits)\n","\n","xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n","loss = tf.reduce_mean(xentropy, name=\"loss\")\n","\n","optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n","training_op = optimizer.minimize(loss)\n","\n","correct = tf.nn.in_top_k(logits, y, 1)\n","accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n","\n","init = tf.global_variables_initializer()\n","\n","dnn_A_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"DNN_A\")\n","restore_saver = tf.train.Saver(var_list={var.op.name: var for var in dnn_A_vars})\n","saver = tf.train.Saver()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dHufhWbNGDax","colab_type":"code","colab":{}},"cell_type":"code","source":["n_epochs = 150\n","batch_size = 50\n","\n","with tf.Session() as sess:\n","    init.run()\n","\n","    for epoch in range(n_epochs):\n","        rnd_idx = np.random.permutation(len(X_train2))\n","        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n","            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n","            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n","        if epoch % 10 == 0:\n","            acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n","            print(epoch, \"Test accuracy:\", acc_test)\n","\n","    save_path = saver.save(sess, \"./my_mnist_model_final.ckpt\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HugiyTJ9GDax","colab_type":"text"},"cell_type":"markdown","source":["Only 94.8% accuracy... So transfer learning helped us reduce the error rate from 5.2% to 3.3% (that's over 36% error reduction). Moreover, the model using transfer learning reached over 96% accuracy in less than 10 epochs.\n","\n","Bottom line: transfer learning does not always work (as we saw in exercise 9), but when it does it can make a big difference. So try it out!"]},{"metadata":{"id":"udxzA2fqGDay","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}